{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization - Break Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intuitions and Workflow\n",
    "\n",
    "Firstly, we have a set $U$ of users, and a set $D$ of items.<br/>Let $R$ of size $|U| \\times |D|$ be the matrix that contains all the ratings that the users have assigned to the items.<br/>We assume that we would like to discover $K$ latent features.<br/>We must then find two matrics matrices P (of size $|U| \\times |K|$) and Q (of size  $|D| \\times |K|$) such that their product apprioximates $|R|$:\n",
    "\n",
    "$$\\mathbf{R} \\approx \\mathbf{P} \\times \\mathbf{Q}^T = \\hat{\\mathbf{R}}$$\n",
    "\n",
    "Each row of $P$ would represent the strength of the associations between **a user and the features**.<br/>Each row of $Q$ would represent the strength of the associations between **an item and the features**.<br/>To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of their vectors:\n",
    "\n",
    "$$\\hat{r}_{ij} = p_i^T q_j = \\sum_{k=1}^k{p_{ik} q_{kj}}$$\n",
    "\n",
    "Now, we have to find a way to obtain $P$ and $Q$. One way to approach this problem is the first intialize the two matrices with some values, calculate how different their product is to $R$, and then try to minimize this difference iteratively through **gradient descent**. The difference here, usually called the error between the estimated rating and the real rating, can be calculated by the following equation for each user-item pair:\n",
    "\n",
    "$$e_{ij}^2 = (r_{ij} - \\hat{r}_{ij})^2 = (r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2$$\n",
    "\n",
    "**Squared error** is considered here. To minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. We know this by getting the gradient at the current valuesby differentiating the above equation with respect to $p_{ik}$ and $q_{kj}$ separately:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = -2(r_{ij} - \\hat{r}_{ij})(q_{kj}) = -2 e_{ij} q_{kj}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial q_{ik}}e_{ij}^2 = -2(r_{ij} - \\hat{r}_{ij})(p_{ik}) = -2 e_{ij} p_{ik}$$\n",
    "\n",
    "The update rules for the values will thus be:\n",
    "\n",
    "$$p’_{ik} = p_{ik} + \\alpha \\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = p_{ik} + 2\\alpha e_{ij} q_{kj}$$\n",
    "\n",
    "$$q’_{kj} = q_{kj} + \\alpha \\frac{\\partial}{\\partial q_{kj}}e_{ij}^2 = q_{kj} + 2\\alpha e_{ij} p_{ik}$$\n",
    "\n",
    "$\\mathbf{\\alpha}$ is the learning rate, which should be small (say 0.0002) to avoid the event of oscillating around the minimum.\n",
    "\n",
    "**Note:** If we find two matrices $\\mathbf{P}$ and $\\mathbf{Q}$ such that $\\mathbf{P} \\times \\mathbf{Q}$ approximates $\\mathbf{R}$, isn’t that our predictions of all the unseen ratings will be zeros? In fact, we are **not** really trying to come up with $\\mathbf{P}$ and $\\mathbf{Q}$ such that we can reproduce $\\mathbf{R}$ exactly. Instead, **we will only try to minimise the errors of the observed user-item pairs**. In other words, if we let $T$ be a set of tuples, each of which is in the form of $(u_i, d_j, r_{ij})$, such that $T$ contains all the observed user-item pairs together with the associated ratings, we are only trying to minimise every $e_{ij}$ for $(u_i, d_j, r_{ij}) \\in T$. (In other words, $T$ is our set of training data.) As for the rest of the unknowns, we will be able to determine their values once the associations between the users, items and features have been learnt.\n",
    "\n",
    "Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.\n",
    "\n",
    "$$E = \\sum_{(u_i, d_j, r_{ij}) \\in T}{e_{ij}} = \\sum_{(u_i,d_j,r_{ij}) \\in T}{(r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2}$$\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Regularization avoids overfitting. It can be done by adding a $\\beta$ parameter to the square error:\n",
    "\n",
    "$$e_{ij}^2 = (r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2 + \\frac{\\beta}{2} \\sum_{k=1}^K{(||P||^2 + ||Q||^2)}$$\n",
    "\n",
    "The $\\beta$ parameter controls the **magnitudes** of the user-feature and item-feature vectors such that $P$ and $Q$ would give predictions of $R$ that are not too large. In practice, $\\beta$ is set to some values in the order of 0.02. The new update rules for this squared error can be obtained by a procedure similar to the one described above. The new update rules are as follows: \n",
    "\n",
    "$$p’_{ik} = p_{ik} + \\alpha \\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = p_{ik} + \\alpha(2 e_{ij} q_{kj} - \\beta p_{ik} )$$\n",
    "\n",
    "$$q’_{kj} = q_{kj} + \\alpha \\frac{\\partial}{\\partial q_{kj}}e_{ij}^2 = q_{kj} + \\alpha(2 e_{ij} p_{ik} - \\beta q_{kj} )$$\n",
    "\n",
    "\n",
    "### Biases\n",
    "\n",
    "Adding biases will better model how a particular user rates items. E.g. A person who is a big movie fan may rate movies more strictly and thus give lower ratings. Biases can be added to the prediction rating as follows:\n",
    "\n",
    "$$\\hat{r}_{ij} = b + bu_i + bd_j + \\sum_{k=1}^k{p_{ik} q_{kj}}$$\n",
    "\n",
    "$\\hat{r}_{ij}$ - prediction<br/>\n",
    "$b$ - global bias (mean of all ratings not including missing ratings)<br/>\n",
    "$bu_i$ - bias for user $i$<br/>\n",
    "$bd_j$ - bias for item $j$<br/>\n",
    "\n",
    "We can derive the update rules for user and item biases as follows:\n",
    "\n",
    "$$bu’_i = bu_i + \\alpha \\times (e_{ij} - \\beta bu_i)$$\n",
    "\n",
    "$$bd’_j = bd_j + \\alpha \\times (e_{ij} - \\beta bd_j)$$\n",
    "\n",
    "\n",
    "Source: http://www.albertauyeung.com/post/python-matrix-factorization/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Create User - Item Ratings Matrix (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>U1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    D1  D2  D3  D4\n",
       "U1   5   3   0   1\n",
       "U2   4   0   0   1\n",
       "U3   1   1   0   5\n",
       "U4   1   0   0   4\n",
       "U5   0   1   5   4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.DataFrame([[5, 3, 0, 1],\n",
    "                        [4, 0, 0, 1],\n",
    "                        [1, 1, 0, 5],\n",
    "                        [1, 0, 0, 4],\n",
    "                        [0, 1, 5, 4]], \n",
    "                       index=['U1','U2','U3','U4','U5'],\n",
    "                       columns=['D1','D2','D3','D4'])\n",
    "ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 0, 1],\n",
       "       [4, 0, 0, 1],\n",
       "       [1, 1, 0, 5],\n",
       "       [1, 0, 0, 4],\n",
       "       [0, 1, 5, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = ratings.values\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 5\n",
      "num_items: 4\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items = R.shape\n",
    "\n",
    "print('num_users:', num_users)\n",
    "print('num_items:', num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Decide the Number of Latent Features (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Initialize User Latent Feature Matrix (P) and Item Latent Feature Matrix (Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.17365756, -0.07654741],\n",
       "       [ 0.1766468 ,  0.16675567],\n",
       "       [-0.62707613,  0.3314999 ],\n",
       "       [-0.90082714, -0.52598719],\n",
       "       [-0.41930046,  0.45092763]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize with random values\n",
    "P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "print(P.shape)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.79468844,  0.092493  ],\n",
       "       [ 0.17407833,  0.27195144],\n",
       "       [-0.28755566,  0.26333669],\n",
       "       [-0.77220423, -0.40805792]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize with random values\n",
    "Q = np.random.normal(scale=1./K, size=(num_items, K))\n",
    "print(Q.shape)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if the product of the 2 matrices will be the same shape as R\n",
    "print(np.dot(P,Q.T).shape == R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Initialize Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "2.769230769230769\n"
     ]
    }
   ],
   "source": [
    "# Initialize biases\n",
    "b_u = np.zeros(num_users)\n",
    "b_i = np.zeros(num_items)\n",
    "b = np.mean(R[np.where(R != 0)])\n",
    "\n",
    "print(b_u)\n",
    "print(b_i)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.62414701, 2.77864361, 2.69913681, 2.66636744],\n",
       "       [2.64427533, 2.84533059, 2.76234787, 2.56477739],\n",
       "       [3.29822234, 2.75022228, 3.03684615, 3.11819045],\n",
       "       [3.43645756, 2.46937331, 2.88975699, 3.67948654],\n",
       "       [3.14415165, 2.81887006, 3.00854878, 2.90901177]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction of the full matrix given our biases above\n",
    "full_matrix = b + b_u[:,np.newaxis] + b_i[np.newaxis:,] + np.dot(P, Q.T)\n",
    "full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 0, 1],\n",
       "       [4, 0, 0, 1],\n",
       "       [1, 1, 0, 5],\n",
       "       [1, 0, 0, 4],\n",
       "       [0, 1, 5, 4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visually compare to R\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. See How Different the PxQ is from R (Calculate Mean Squared Error)\n",
    "\n",
    "Mean Squared Error (MSE) is calculated by the following formula:\n",
    "\n",
    "$$e_{ij}^2 = (r_{ij} - \\hat{r}_{ij})^2 = (r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 2 2 2 3 3 4 4 4]\n",
      "[0 1 3 0 3 0 1 3 0 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Indices in R that are non-zero\n",
    "xs, ys = R.nonzero()\n",
    "\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error\n",
    "error = 0\n",
    "for x, y in zip(xs, ys):\n",
    "    error += (R[x,y] - full_matrix[x,y])**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 39.14676121360276\n",
      "root mean squared error: 6.2567372658281535\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error:', error)\n",
    "print('root mean squared error:', np.sqrt(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Reduce this Error with Gradient Descent (1 Iteration for now)\n",
    "\n",
    "Updating latent feature matrices $P$ and $Q$:\n",
    "\n",
    "$$p’_{ik} = p_{ik} + \\alpha \\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = p_{ik} + \\alpha(2 e_{ij} q_{kj} - \\beta p_{ik} )$$\n",
    "\n",
    "$$q’_{kj} = q_{kj} + \\alpha \\frac{\\partial}{\\partial q_{kj}}e_{ij}^2 = q_{kj} + \\alpha(2 e_{ij} p_{ik} - \\beta q_{kj} )$$\n",
    "\n",
    "Updating biases $bu_i$ and $bd_j$:\n",
    "\n",
    "$$bu’_i = bu_i + \\alpha \\times (e_{ij} - \\beta bu_i)$$\n",
    "\n",
    "$$bd’_j = bd_j + \\alpha \\times (e_{ij} - \\beta bd_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 5),\n",
       " (0, 1, 3),\n",
       " (0, 3, 1),\n",
       " (1, 0, 4),\n",
       " (1, 3, 1),\n",
       " (2, 0, 1),\n",
       " (2, 1, 1),\n",
       " (2, 3, 5),\n",
       " (3, 0, 1),\n",
       " (3, 3, 4),\n",
       " (4, 1, 1),\n",
       " (4, 2, 5),\n",
       " (4, 3, 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training samples - tuples of user, item, rating if rating is not 0\n",
    "samples = [(u, i, R[u,i]) for u in range(num_users) for i in range(num_items) if R[u,i]>0]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alpha - learning rate\n",
    "# beta - regularization parameter\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "\n",
    "# Compute error, use update rule on biases, then update latent feature matrices\n",
    "for u, i, r in samples:\n",
    "    \n",
    "    # For each training sample, compute prediction and error\n",
    "    # For each rating prediction, we will add bias, user bias and item bias\n",
    "    prediction = b + b_u[u] + b_i[i] + np.dot(P[u,:], Q[i,:])\n",
    "    e = (r - prediction)\n",
    "    \n",
    "    # Update biases\n",
    "    b_u[u] += alpha * (e - beta * b_u[u])\n",
    "    b_i[i] += alpha * (e - beta * b_i[i])\n",
    "    \n",
    "    # Update user and item latent feature matrices\n",
    "    P[u, :] += alpha * (e * Q[i, :] - beta * P[u, :])\n",
    "    Q[i, :] += alpha * (e * P[u, :] - beta * Q[i, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated user biases: [ 0.03403453 -0.04151773 -0.14011109 -0.18565306  0.13877416]\n",
      "updated item biases: [-0.16077853 -0.31257136  0.21525295  0.06332075]\n"
     ]
    }
   ],
   "source": [
    "# See how our biases have been updated\n",
    "print('updated user biases:', b_u)\n",
    "print('updated item biases:', b_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated P - user latent features matrix\n",
      "[[ 0.14253307  0.0292047 ]\n",
      " [ 0.20986517  0.2388028 ]\n",
      " [-0.67282281  0.13938515]\n",
      " [-0.79384293 -0.55382042]\n",
      " [-0.6098567   0.4307764 ]]\n",
      "\n",
      "updated Q - item latent features matrix\n",
      "[[-0.49478692  0.14824369]\n",
      " [ 0.31509836  0.1641975 ]\n",
      " [-0.39907462  0.36407304]\n",
      " [-1.11360151 -0.40617986]]\n"
     ]
    }
   ],
   "source": [
    "# See how our latent feature matrices P and Q have been updated\n",
    "print('updated P - user latent features matrix')\n",
    "print(P)\n",
    "print()\n",
    "print('updated Q - item latent features matrix')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8. Re-calculate Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.57629268, 2.54040122, 2.97226956, 2.69599864],\n",
       "       [2.49849697, 2.52048067, 2.94615579, 2.46033073],\n",
       "       [2.82190804, 2.12742965, 3.1636255 , 3.38508149],\n",
       "       [2.73348189, 1.92993182, 2.91400213, 3.75587385],\n",
       "       [3.1128354 , 2.47400113, 3.52347028, 3.47549033]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction of the full matrix given our biases above\n",
    "full_matrix_2 = b + b_u[:,np.newaxis] + b_i[np.newaxis:,] + np.dot(P, Q.T)\n",
    "full_matrix_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 2 2 2 3 3 4 4 4]\n",
      "[0 1 3 0 3 0 1 3 0 3 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Indices in R that are non-zero\n",
    "xs, ys = R.nonzero()\n",
    "\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error\n",
    "error_2 = 0\n",
    "for x, y in zip(xs, ys):\n",
    "    error_2 += (R[x,y] - full_matrix_2[x,y])**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Iterations\n",
      "mse: 39.14676121360276\n",
      "rmse: 6.2567372658281535\n",
      "\n",
      "1 Iterations\n",
      "mse: 28.23997195578064\n",
      "rmse: 5.314129463588617\n"
     ]
    }
   ],
   "source": [
    "# See the effect of gradient descent on our mse and rmse after 1 iteration\n",
    "print('0 Iterations')\n",
    "print('mse:', error)\n",
    "print('rmse:', np.sqrt(error))\n",
    "print()\n",
    "print('1 Iterations')\n",
    "print('mse:', error_2)\n",
    "print('rmse:', np.sqrt(error_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Matrix at 0 Iterations\n",
      "[[2.62414701 2.77864361 2.69913681 2.66636744]\n",
      " [2.64427533 2.84533059 2.76234787 2.56477739]\n",
      " [3.29822234 2.75022228 3.03684615 3.11819045]\n",
      " [3.43645756 2.46937331 2.88975699 3.67948654]\n",
      " [3.14415165 2.81887006 3.00854878 2.90901177]]\n",
      "\n",
      "Full Matrix at 1 Iterations\n",
      "[[2.57629268 2.54040122 2.97226956 2.69599864]\n",
      " [2.49849697 2.52048067 2.94615579 2.46033073]\n",
      " [2.82190804 2.12742965 3.1636255  3.38508149]\n",
      " [2.73348189 1.92993182 2.91400213 3.75587385]\n",
      " [3.1128354  2.47400113 3.52347028 3.47549033]]\n"
     ]
    }
   ],
   "source": [
    "# See the effect of gradient descent on our full matrix after 1 iteration\n",
    "print('Full Matrix at 0 Iterations')\n",
    "print(full_matrix)\n",
    "print()\n",
    "print('Full Matrix at 1 Iterations')\n",
    "print(full_matrix_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** In section 1, we have seen how gradient descent has effectively decreased the mean squared error of our predictions after 1 iteration. It can be imagined that this process can be repeated for a number of iterations such that we get the lowers RMSE possible. In order to implement this iterative process, we must first consolidate the method above into functions and further into a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Consolidating the Process\n",
    "\n",
    "The first function, train() is the main function in which the other helper functions will fall into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(R, K=2, iterations=10, alpha=0.1, beta=0.01):\n",
    "    '''\n",
    "    Function that runs the training process. Steps:\n",
    "    \n",
    "    1. Initialize user and item latent features matrices.\n",
    "    2. Initialize global, user and item biases.\n",
    "    3. Implement gradient descent.\n",
    "    4. Calculate RMSE at each interation.\n",
    "    '''    \n",
    "    \n",
    "    # Initialize P and Q with random values - CLASS ATTR\n",
    "    num_users, num_items = R.shape\n",
    "    P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "    Q = np.random.normal(scale=1./K, size=(num_items, K))\n",
    "\n",
    "    # Initialize biases - CLASS ATTR\n",
    "    b_u = np.zeros(num_users)\n",
    "    b_i = np.zeros(num_items)\n",
    "    b = np.mean(R[np.where(R != 0)])\n",
    "\n",
    "    # Training samples - tuples of user, item, rating if rating is not 0\n",
    "    samples = [(u, i, R[u,i]) for u in range(num_users) for i in range(num_items) if R[u,i]>0]\n",
    "\n",
    "    # Stochastic gradient descent for given number of iterations\n",
    "    training_process = []\n",
    "    for i in range(iterations):\n",
    "        # Shuffles the training samples\n",
    "        np.random.shuffle(samples)\n",
    "        # Gradient descent - at each iteration, biases and latent feature matrices updated\n",
    "        gradient_descent(P = P, Q = Q, b = b, b_i = b_i, b_u = b_u, \n",
    "                         alpha = alpha, beta = beta, samples = samples)\n",
    "        # RMSE calculated\n",
    "        root_mse = rmse(R, P = P, Q = Q, b = b, b_u = b_u, b_i = b_i)\n",
    "        training_process.append((i, root_mse))\n",
    "        # Print every 10 iterations\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Iteration: %d ; error = %.4f\" % (i+1, root_mse))\n",
    "    \n",
    "    # Store latent features matrices in a tuple - CLASS ATTRR        \n",
    "    latent_ft_mats = (P, Q)\n",
    "    \n",
    "    # Store biases in a tuple - CLASS ATTR\n",
    "    biases = (b_u, b_i, b)\n",
    "    \n",
    "    # Full Matrix - CLASS ATTR\n",
    "    final_mat = full_matrix(P = P, Q = Q, b = b, b_u = b_u, b_i = b_i)\n",
    "\n",
    "    return training_process, latent_ft_mats, biases, final_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(P = P, Q = Q, b = b, b_i = b_i, b_u = b_u, \n",
    "                     alpha = alpha, beta = beta, samples = samples):\n",
    "    '''\n",
    "    Function that runs gradient descent. Steps:\n",
    "    \n",
    "    1. Makes a prediction for each training rating.\n",
    "    2. Get the error for each prediction.\n",
    "    3. Update biases based on error, learning rate (alpha) and regularization parameter (beta).\n",
    "    4. Update latent feature matrices based on the same.\n",
    "    '''\n",
    "\n",
    "    # Compute error, use update rule on biases, then update latent feature matrices\n",
    "    for u, i, r in samples:\n",
    "\n",
    "        # For each training sample, compute prediction and error\n",
    "        # For each rating prediction, we will add bias, user bias and item bias\n",
    "        prediction = b + b_u[u] + b_i[i] + np.dot(P[u,:], Q[i,:])\n",
    "        e = (r - prediction)\n",
    "\n",
    "        # Update biases\n",
    "        b_u[u] += alpha * (e - beta * b_u[u])\n",
    "        b_i[i] += alpha * (e - beta * b_i[i])\n",
    "\n",
    "        # Update user and item latent feature matrices\n",
    "        P[u, :] += alpha * (e * Q[i, :] - beta * P[u, :])\n",
    "        Q[i, :] += alpha * (e * P[u, :] - beta * Q[i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_matrix(P = P, Q = Q, b = b, b_u = b_u, b_i = b_i):\n",
    "    '''\n",
    "    Function that computes the full matrix based on updated P, Q and biases. \n",
    "    '''\n",
    "    return b + b_u[:,np.newaxis] + b_i[np.newaxis:,] + np.dot(P, Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(R, P = P, Q = Q, b = b, b_u = b_u, b_i = b_i):\n",
    "    '''\n",
    "    Function that calculates the total root mean squared error.\n",
    "    '''    \n",
    "    \n",
    "    # Predict full matrix\n",
    "    predicted = full_matrix(b = b, b_u = b_u, b_i = b_i, P = P, Q = Q)\n",
    "    \n",
    "    # Calculate error for ratings that are not zero (this is the only way to score)\n",
    "    xs, ys = R.nonzero()\n",
    "    error = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        error += (R[x,y] - predicted[x,y])**2\n",
    "        \n",
    "    return np.sqrt(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Testing out our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 0, 1],\n",
       "       [4, 0, 0, 1],\n",
       "       [1, 1, 0, 5],\n",
       "       [1, 0, 0, 4],\n",
       "       [0, 1, 5, 4]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.DataFrame([[5, 3, 0, 1],\n",
    "                        [4, 0, 0, 1],\n",
    "                        [1, 1, 0, 5],\n",
    "                        [1, 0, 0, 4],\n",
    "                        [0, 1, 5, 4]], \n",
    "                       index=['U1','U2','U3','U4','U5'],\n",
    "                       columns=['D1','D2','D3','D4'])\n",
    "\n",
    "R = ratings.values\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; error = 0.4125\n",
      "Iteration: 20 ; error = 0.0898\n",
      "Iteration: 30 ; error = 0.0439\n",
      "Iteration: 40 ; error = 0.0379\n",
      "Iteration: 50 ; error = 0.0348\n",
      "Iteration: 60 ; error = 0.0372\n",
      "Iteration: 70 ; error = 0.0378\n",
      "Iteration: 80 ; error = 0.0355\n",
      "Iteration: 90 ; error = 0.0398\n",
      "Iteration: 100 ; error = 0.0374\n",
      "Iteration: 110 ; error = 0.0366\n",
      "Iteration: 120 ; error = 0.0370\n",
      "Iteration: 130 ; error = 0.0335\n",
      "Iteration: 140 ; error = 0.0372\n",
      "Iteration: 150 ; error = 0.0382\n",
      "Iteration: 160 ; error = 0.0354\n",
      "Iteration: 170 ; error = 0.0353\n",
      "Iteration: 180 ; error = 0.0357\n",
      "Iteration: 190 ; error = 0.0360\n",
      "Iteration: 200 ; error = 0.0359\n",
      "Iteration: 210 ; error = 0.0377\n",
      "Iteration: 220 ; error = 0.0381\n",
      "Iteration: 230 ; error = 0.0351\n",
      "Iteration: 240 ; error = 0.0341\n",
      "Iteration: 250 ; error = 0.0368\n",
      "Iteration: 260 ; error = 0.0387\n",
      "Iteration: 270 ; error = 0.0362\n",
      "Iteration: 280 ; error = 0.0335\n",
      "Iteration: 290 ; error = 0.0361\n",
      "Iteration: 300 ; error = 0.0386\n",
      "Iteration: 310 ; error = 0.0355\n",
      "Iteration: 320 ; error = 0.0392\n",
      "Iteration: 330 ; error = 0.0388\n",
      "Iteration: 340 ; error = 0.0381\n",
      "Iteration: 350 ; error = 0.0373\n",
      "Iteration: 360 ; error = 0.0360\n",
      "Iteration: 370 ; error = 0.0383\n",
      "Iteration: 380 ; error = 0.0373\n",
      "Iteration: 390 ; error = 0.0371\n",
      "Iteration: 400 ; error = 0.0362\n",
      "Iteration: 410 ; error = 0.0360\n",
      "Iteration: 420 ; error = 0.0357\n",
      "Iteration: 430 ; error = 0.0361\n",
      "Iteration: 440 ; error = 0.0381\n",
      "Iteration: 450 ; error = 0.0355\n",
      "Iteration: 460 ; error = 0.0358\n",
      "Iteration: 470 ; error = 0.0387\n",
      "Iteration: 480 ; error = 0.0349\n",
      "Iteration: 490 ; error = 0.0358\n",
      "Iteration: 500 ; error = 0.0389\n",
      "Iteration: 510 ; error = 0.0367\n",
      "Iteration: 520 ; error = 0.0351\n",
      "Iteration: 530 ; error = 0.0361\n",
      "Iteration: 540 ; error = 0.0366\n",
      "Iteration: 550 ; error = 0.0359\n",
      "Iteration: 560 ; error = 0.0375\n",
      "Iteration: 570 ; error = 0.0353\n",
      "Iteration: 580 ; error = 0.0385\n",
      "Iteration: 590 ; error = 0.0357\n",
      "Iteration: 600 ; error = 0.0381\n",
      "Iteration: 610 ; error = 0.0385\n",
      "Iteration: 620 ; error = 0.0366\n",
      "Iteration: 630 ; error = 0.0355\n",
      "Iteration: 640 ; error = 0.0399\n",
      "Iteration: 650 ; error = 0.0362\n",
      "Iteration: 660 ; error = 0.0338\n",
      "Iteration: 670 ; error = 0.0360\n",
      "Iteration: 680 ; error = 0.0369\n",
      "Iteration: 690 ; error = 0.0330\n",
      "Iteration: 700 ; error = 0.0359\n",
      "Iteration: 710 ; error = 0.0353\n",
      "Iteration: 720 ; error = 0.0350\n",
      "Iteration: 730 ; error = 0.0355\n",
      "Iteration: 740 ; error = 0.0360\n",
      "Iteration: 750 ; error = 0.0356\n",
      "Iteration: 760 ; error = 0.0385\n",
      "Iteration: 770 ; error = 0.0394\n",
      "Iteration: 780 ; error = 0.0378\n",
      "Iteration: 790 ; error = 0.0386\n",
      "Iteration: 800 ; error = 0.0397\n",
      "Iteration: 810 ; error = 0.0363\n",
      "Iteration: 820 ; error = 0.0383\n",
      "Iteration: 830 ; error = 0.0363\n",
      "Iteration: 840 ; error = 0.0368\n",
      "Iteration: 850 ; error = 0.0393\n",
      "Iteration: 860 ; error = 0.0362\n",
      "Iteration: 870 ; error = 0.0380\n",
      "Iteration: 880 ; error = 0.0390\n",
      "Iteration: 890 ; error = 0.0358\n",
      "Iteration: 900 ; error = 0.0359\n",
      "Iteration: 910 ; error = 0.0359\n",
      "Iteration: 920 ; error = 0.0385\n",
      "Iteration: 930 ; error = 0.0397\n",
      "Iteration: 940 ; error = 0.0372\n",
      "Iteration: 950 ; error = 0.0387\n",
      "Iteration: 960 ; error = 0.0391\n",
      "Iteration: 970 ; error = 0.0375\n",
      "Iteration: 980 ; error = 0.0364\n",
      "Iteration: 990 ; error = 0.0338\n",
      "Iteration: 1000 ; error = 0.0396\n"
     ]
    }
   ],
   "source": [
    "train_process, latent_ft_mats, biases, final_mat = train(R, K=2, iterations=1000, alpha=0.1, beta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5.256810847147864),\n",
       " (1, 4.3178455976145615),\n",
       " (2, 3.094749106077734),\n",
       " (3, 2.1903984115173993),\n",
       " (4, 1.5500027956076419),\n",
       " (5, 1.1450396849027542),\n",
       " (6, 0.8656224818734857),\n",
       " (7, 0.6581412325116689),\n",
       " (8, 0.523116883039666),\n",
       " (9, 0.41247411284436175),\n",
       " (10, 0.3355972327013934),\n",
       " (11, 0.27468420405615557),\n",
       " (12, 0.23691368145865768),\n",
       " (13, 0.20020904457535302),\n",
       " (14, 0.1675035382516854),\n",
       " (15, 0.15019777038000817),\n",
       " (16, 0.13384031648996222),\n",
       " (17, 0.11303552631290296),\n",
       " (18, 0.10061168758862817),\n",
       " (19, 0.08983765012907471),\n",
       " (20, 0.08117510885476409),\n",
       " (21, 0.07573609063117957),\n",
       " (22, 0.06869594155242598),\n",
       " (23, 0.06170013384836862),\n",
       " (24, 0.05690406177455608),\n",
       " (25, 0.05365989904064138),\n",
       " (26, 0.05014634144563248),\n",
       " (27, 0.04531357712198612),\n",
       " (28, 0.046839157553227026),\n",
       " (29, 0.04393986919337992),\n",
       " (30, 0.04509148178117489),\n",
       " (31, 0.04081998904580534),\n",
       " (32, 0.04043774317856896),\n",
       " (33, 0.040647739922707544),\n",
       " (34, 0.03868827235332857),\n",
       " (35, 0.039543599186509536),\n",
       " (36, 0.040467507825318626),\n",
       " (37, 0.04229020232247969),\n",
       " (38, 0.03630902332088308),\n",
       " (39, 0.03789316838010059),\n",
       " (40, 0.03849445207307598),\n",
       " (41, 0.035630761587604275),\n",
       " (42, 0.03404343103598807),\n",
       " (43, 0.03724624929459351),\n",
       " (44, 0.03688192119357215),\n",
       " (45, 0.034301926004017415),\n",
       " (46, 0.037368712707855946),\n",
       " (47, 0.03860966902035975),\n",
       " (48, 0.03722816216813247),\n",
       " (49, 0.03475195756293231),\n",
       " (50, 0.03368079677736859),\n",
       " (51, 0.03627764162103471),\n",
       " (52, 0.037321018215201895),\n",
       " (53, 0.03644634563607954),\n",
       " (54, 0.037597524246086464),\n",
       " (55, 0.03766695057508715),\n",
       " (56, 0.0378039545147149),\n",
       " (57, 0.035094481158892625),\n",
       " (58, 0.03720793503937768),\n",
       " (59, 0.03715884245132663),\n",
       " (60, 0.037978207387594214),\n",
       " (61, 0.037978122707941195),\n",
       " (62, 0.03824705442369594),\n",
       " (63, 0.034782847090195995),\n",
       " (64, 0.035546680281103096),\n",
       " (65, 0.03548991511257206),\n",
       " (66, 0.03740864344169519),\n",
       " (67, 0.03691373233432962),\n",
       " (68, 0.037348180818425614),\n",
       " (69, 0.037754516364027324),\n",
       " (70, 0.03804136414025756),\n",
       " (71, 0.03773028983579086),\n",
       " (72, 0.038278731355849575),\n",
       " (73, 0.03626405834753016),\n",
       " (74, 0.03882089400917553),\n",
       " (75, 0.037014087885787986),\n",
       " (76, 0.037247971370702085),\n",
       " (77, 0.03527104870186869),\n",
       " (78, 0.03594873172148534),\n",
       " (79, 0.03551832836172152),\n",
       " (80, 0.03582891243018405),\n",
       " (81, 0.03715389415808961),\n",
       " (82, 0.034221635996978146),\n",
       " (83, 0.03604699221034395),\n",
       " (84, 0.03233466606202694),\n",
       " (85, 0.035042273407444466),\n",
       " (86, 0.037357521220108554),\n",
       " (87, 0.0359916840820541),\n",
       " (88, 0.03426768424453916),\n",
       " (89, 0.0397738575816772),\n",
       " (90, 0.03441117383952465),\n",
       " (91, 0.033771396260695453),\n",
       " (92, 0.03541945479979051),\n",
       " (93, 0.0388678148296965),\n",
       " (94, 0.039327911633899156),\n",
       " (95, 0.03477024821005654),\n",
       " (96, 0.03637147524132354),\n",
       " (97, 0.035661134377605035),\n",
       " (98, 0.03457963479450037),\n",
       " (99, 0.03738414866668012),\n",
       " (100, 0.035181206561895065),\n",
       " (101, 0.037642493805948925),\n",
       " (102, 0.03716937252352759),\n",
       " (103, 0.03808586639339789),\n",
       " (104, 0.03624602335093053),\n",
       " (105, 0.03860052619140979),\n",
       " (106, 0.0350433293202759),\n",
       " (107, 0.03642036527539798),\n",
       " (108, 0.03590540211701548),\n",
       " (109, 0.03656771912815868),\n",
       " (110, 0.037665270634844275),\n",
       " (111, 0.03508674909768973),\n",
       " (112, 0.03656696185060263),\n",
       " (113, 0.03598839168909624),\n",
       " (114, 0.034358068945602616),\n",
       " (115, 0.03447675451734761),\n",
       " (116, 0.03517889543803891),\n",
       " (117, 0.036868467391510625),\n",
       " (118, 0.038637721191631634),\n",
       " (119, 0.0370246581577472),\n",
       " (120, 0.03518937104042907),\n",
       " (121, 0.04006032468818972),\n",
       " (122, 0.03946460291486016),\n",
       " (123, 0.03880085285812138),\n",
       " (124, 0.034787885327988044),\n",
       " (125, 0.037768954921802315),\n",
       " (126, 0.03605285691637138),\n",
       " (127, 0.03523222025266713),\n",
       " (128, 0.03585045995663899),\n",
       " (129, 0.03346410204539576),\n",
       " (130, 0.038650176048665705),\n",
       " (131, 0.03501263477129403),\n",
       " (132, 0.03444730796273575),\n",
       " (133, 0.037221368815492546),\n",
       " (134, 0.03707373354166199),\n",
       " (135, 0.034994492977581323),\n",
       " (136, 0.03712428690810995),\n",
       " (137, 0.03915207570460853),\n",
       " (138, 0.036429458508524644),\n",
       " (139, 0.03721833930808501),\n",
       " (140, 0.036045656046459026),\n",
       " (141, 0.0362147621628771),\n",
       " (142, 0.03521468660519684),\n",
       " (143, 0.03626927963385018),\n",
       " (144, 0.03655402112536095),\n",
       " (145, 0.03701530454918555),\n",
       " (146, 0.03713671403004732),\n",
       " (147, 0.03447143312074798),\n",
       " (148, 0.038244645157276394),\n",
       " (149, 0.03818868887074271),\n",
       " (150, 0.03779887140758158),\n",
       " (151, 0.03898475495653544),\n",
       " (152, 0.03756203711198118),\n",
       " (153, 0.03411475096208013),\n",
       " (154, 0.03667026284578049),\n",
       " (155, 0.03630007368608179),\n",
       " (156, 0.035029898016603174),\n",
       " (157, 0.036063859948894),\n",
       " (158, 0.0357969454209155),\n",
       " (159, 0.035429891150853675),\n",
       " (160, 0.03864163970924901),\n",
       " (161, 0.03347147275661997),\n",
       " (162, 0.033258323941613196),\n",
       " (163, 0.0384808214404949),\n",
       " (164, 0.03464106205759277),\n",
       " (165, 0.03759618853030589),\n",
       " (166, 0.03835208483883779),\n",
       " (167, 0.03675865280835186),\n",
       " (168, 0.03726592855043948),\n",
       " (169, 0.03533934891421553),\n",
       " (170, 0.035650619510579094),\n",
       " (171, 0.03352076390372281),\n",
       " (172, 0.036971776589061325),\n",
       " (173, 0.034912668606785054),\n",
       " (174, 0.03722969357265404),\n",
       " (175, 0.035560505850668146),\n",
       " (176, 0.037755614222024766),\n",
       " (177, 0.0367965399923772),\n",
       " (178, 0.038454967481158564),\n",
       " (179, 0.03571657905031277),\n",
       " (180, 0.040253592437321455),\n",
       " (181, 0.04040436188481877),\n",
       " (182, 0.03874128186685581),\n",
       " (183, 0.03540189999743859),\n",
       " (184, 0.03566945344212593),\n",
       " (185, 0.03716699719039574),\n",
       " (186, 0.03368297465339597),\n",
       " (187, 0.03295284992680652),\n",
       " (188, 0.03637550365760674),\n",
       " (189, 0.0359649091074458),\n",
       " (190, 0.03637534580650389),\n",
       " (191, 0.038135677327459955),\n",
       " (192, 0.037426165601173435),\n",
       " (193, 0.03536848024360699),\n",
       " (194, 0.038034358880454554),\n",
       " (195, 0.03530014049406033),\n",
       " (196, 0.037207729875255016),\n",
       " (197, 0.03504097449543188),\n",
       " (198, 0.03882483397346553),\n",
       " (199, 0.03592066946764665),\n",
       " (200, 0.036775018925839585),\n",
       " (201, 0.03762665875685219),\n",
       " (202, 0.03741080390537896),\n",
       " (203, 0.035170686611122305),\n",
       " (204, 0.03395865048582601),\n",
       " (205, 0.038410914093949124),\n",
       " (206, 0.034241895297767506),\n",
       " (207, 0.03707552055618648),\n",
       " (208, 0.03414015217477817),\n",
       " (209, 0.03773758545914298),\n",
       " (210, 0.03566400176641405),\n",
       " (211, 0.037869433841854094),\n",
       " (212, 0.03549492864472136),\n",
       " (213, 0.04070088000071617),\n",
       " (214, 0.0380153858333316),\n",
       " (215, 0.03733847095063242),\n",
       " (216, 0.03622739284196719),\n",
       " (217, 0.03681510001614645),\n",
       " (218, 0.036426758138486975),\n",
       " (219, 0.03808044368492337),\n",
       " (220, 0.03713237108155274),\n",
       " (221, 0.03837806837531412),\n",
       " (222, 0.03873531739764993),\n",
       " (223, 0.03883071421449122),\n",
       " (224, 0.03806026538048251),\n",
       " (225, 0.036725596428602555),\n",
       " (226, 0.03502559484508613),\n",
       " (227, 0.03784353064968441),\n",
       " (228, 0.03575748503169414),\n",
       " (229, 0.03512235993074086),\n",
       " (230, 0.03619713126156008),\n",
       " (231, 0.03673067931988567),\n",
       " (232, 0.03814204764175613),\n",
       " (233, 0.03722168605876775),\n",
       " (234, 0.037519961306346286),\n",
       " (235, 0.037360898310003415),\n",
       " (236, 0.0383576416027927),\n",
       " (237, 0.039203210100426225),\n",
       " (238, 0.03631638074608167),\n",
       " (239, 0.03412366690879927),\n",
       " (240, 0.03877135071632152),\n",
       " (241, 0.03483741550082523),\n",
       " (242, 0.03867904716241914),\n",
       " (243, 0.03620347497107418),\n",
       " (244, 0.0363054133908048),\n",
       " (245, 0.03971183688683308),\n",
       " (246, 0.03663791466417396),\n",
       " (247, 0.03796512155773407),\n",
       " (248, 0.034589353818778555),\n",
       " (249, 0.036836488771510335),\n",
       " (250, 0.03654101717478903),\n",
       " (251, 0.03622393407631669),\n",
       " (252, 0.03842313808748664),\n",
       " (253, 0.03705209240335273),\n",
       " (254, 0.03886983850781844),\n",
       " (255, 0.038044465442404835),\n",
       " (256, 0.0388271069297917),\n",
       " (257, 0.032746100890670655),\n",
       " (258, 0.03617158666403126),\n",
       " (259, 0.03871445202032594),\n",
       " (260, 0.03480168439014581),\n",
       " (261, 0.04007567818596217),\n",
       " (262, 0.03842698159921489),\n",
       " (263, 0.038534591331422105),\n",
       " (264, 0.037923636691339815),\n",
       " (265, 0.03799025389510275),\n",
       " (266, 0.03530977768081075),\n",
       " (267, 0.036616669093188864),\n",
       " (268, 0.03720310154177874),\n",
       " (269, 0.036172572707796155),\n",
       " (270, 0.033290968765979874),\n",
       " (271, 0.032036462034940275),\n",
       " (272, 0.03714468054274269),\n",
       " (273, 0.03870059984543479),\n",
       " (274, 0.035403477367649185),\n",
       " (275, 0.03907169157922339),\n",
       " (276, 0.039894057540986336),\n",
       " (277, 0.03789110616986607),\n",
       " (278, 0.03695830265759822),\n",
       " (279, 0.03350596807086063),\n",
       " (280, 0.03447793753582229),\n",
       " (281, 0.03719811160557352),\n",
       " (282, 0.03948777091270968),\n",
       " (283, 0.04037493706366109),\n",
       " (284, 0.03917107207945896),\n",
       " (285, 0.037061205511878886),\n",
       " (286, 0.038182861885050885),\n",
       " (287, 0.03598875247926954),\n",
       " (288, 0.03487170611802313),\n",
       " (289, 0.03611693626704121),\n",
       " (290, 0.03542271374784204),\n",
       " (291, 0.03506574374935851),\n",
       " (292, 0.037556770014492966),\n",
       " (293, 0.03633847818420847),\n",
       " (294, 0.03842416684679623),\n",
       " (295, 0.03696443426577246),\n",
       " (296, 0.03932808731130638),\n",
       " (297, 0.036056783206777444),\n",
       " (298, 0.03434547456920692),\n",
       " (299, 0.03858527110204795),\n",
       " (300, 0.03260445527020754),\n",
       " (301, 0.039679651996966415),\n",
       " (302, 0.03931144667444979),\n",
       " (303, 0.0392764090708471),\n",
       " (304, 0.040413335936071354),\n",
       " (305, 0.035278524197995514),\n",
       " (306, 0.0367801881112499),\n",
       " (307, 0.03796729725530044),\n",
       " (308, 0.03461197820315072),\n",
       " (309, 0.03547570595161161),\n",
       " (310, 0.03815916471303252),\n",
       " (311, 0.03596611000700723),\n",
       " (312, 0.03707548643721767),\n",
       " (313, 0.034818539136221384),\n",
       " (314, 0.03671291807489913),\n",
       " (315, 0.0362106383978596),\n",
       " (316, 0.03698563784976238),\n",
       " (317, 0.03816277931120132),\n",
       " (318, 0.039043796892907046),\n",
       " (319, 0.03923209415309427),\n",
       " (320, 0.035715827210342624),\n",
       " (321, 0.038343125856020066),\n",
       " (322, 0.03419433426804369),\n",
       " (323, 0.03581860342758118),\n",
       " (324, 0.03554887924434693),\n",
       " (325, 0.03566609104031543),\n",
       " (326, 0.036266412409237316),\n",
       " (327, 0.03896200452408405),\n",
       " (328, 0.03295148104365611),\n",
       " (329, 0.038750553719459783),\n",
       " (330, 0.03837234928432785),\n",
       " (331, 0.0361746645976753),\n",
       " (332, 0.03714841666753191),\n",
       " (333, 0.03649027990414076),\n",
       " (334, 0.036016164747748075),\n",
       " (335, 0.033953709920144896),\n",
       " (336, 0.03757966758646646),\n",
       " (337, 0.035472358515999385),\n",
       " (338, 0.03567702742871297),\n",
       " (339, 0.03807148914354915),\n",
       " (340, 0.03957161138565761),\n",
       " (341, 0.03866594495875226),\n",
       " (342, 0.03562643028503946),\n",
       " (343, 0.03625425169475783),\n",
       " (344, 0.035845084396256735),\n",
       " (345, 0.03852757722134461),\n",
       " (346, 0.035335263109731285),\n",
       " (347, 0.03928788558777535),\n",
       " (348, 0.03372973239648519),\n",
       " (349, 0.03728191562733041),\n",
       " (350, 0.034090883683961896),\n",
       " (351, 0.037226233350381915),\n",
       " (352, 0.03422790950566922),\n",
       " (353, 0.035666006831969535),\n",
       " (354, 0.037975808267996077),\n",
       " (355, 0.03787520136167897),\n",
       " (356, 0.038266689197008354),\n",
       " (357, 0.03592739324646015),\n",
       " (358, 0.03538256370230805),\n",
       " (359, 0.0360357099019621),\n",
       " (360, 0.037955254898067656),\n",
       " (361, 0.037854832146596136),\n",
       " (362, 0.03656218301604462),\n",
       " (363, 0.03892209733162377),\n",
       " (364, 0.03424299600771244),\n",
       " (365, 0.03676575508787314),\n",
       " (366, 0.036248197816492286),\n",
       " (367, 0.0350295212917898),\n",
       " (368, 0.0359282639806181),\n",
       " (369, 0.03828027281151285),\n",
       " (370, 0.03887668043755846),\n",
       " (371, 0.03800998552909741),\n",
       " (372, 0.039615918230580624),\n",
       " (373, 0.03739417048037713),\n",
       " (374, 0.036361108136733274),\n",
       " (375, 0.036611927369803246),\n",
       " (376, 0.03705542274334529),\n",
       " (377, 0.03450839580241342),\n",
       " (378, 0.03646356133711614),\n",
       " (379, 0.03725616687351718),\n",
       " (380, 0.03780888361125131),\n",
       " (381, 0.037275977476517945),\n",
       " (382, 0.03757422127382692),\n",
       " (383, 0.03767983147029177),\n",
       " (384, 0.038623813144849924),\n",
       " (385, 0.036972783801223506),\n",
       " (386, 0.03695458790160422),\n",
       " (387, 0.037925433902830714),\n",
       " (388, 0.03614491037895657),\n",
       " (389, 0.03709060658925172),\n",
       " (390, 0.035447714528035926),\n",
       " (391, 0.038055119097661745),\n",
       " (392, 0.03740645969696093),\n",
       " (393, 0.03850695775322168),\n",
       " (394, 0.03545530920025277),\n",
       " (395, 0.03675014138479887),\n",
       " (396, 0.033985301627366085),\n",
       " (397, 0.035509794382649194),\n",
       " (398, 0.03783248078652312),\n",
       " (399, 0.03619758263690687),\n",
       " (400, 0.035371788829341326),\n",
       " (401, 0.037056237169510904),\n",
       " (402, 0.035638260579079255),\n",
       " (403, 0.04050484210803147),\n",
       " (404, 0.03637200625760273),\n",
       " (405, 0.036633824760708915),\n",
       " (406, 0.03553240115386795),\n",
       " (407, 0.038873308240753364),\n",
       " (408, 0.03742148194426767),\n",
       " (409, 0.03604964202100048),\n",
       " (410, 0.03545799266150726),\n",
       " (411, 0.03526186751386179),\n",
       " (412, 0.03434613894002077),\n",
       " (413, 0.03894472253900471),\n",
       " (414, 0.036914860555362236),\n",
       " (415, 0.03779666489813827),\n",
       " (416, 0.036274486189540545),\n",
       " (417, 0.03743882695944871),\n",
       " (418, 0.038378493650427274),\n",
       " (419, 0.035726536051600345),\n",
       " (420, 0.03829670065019575),\n",
       " (421, 0.03880243331178879),\n",
       " (422, 0.03444048217538678),\n",
       " (423, 0.03933681468360981),\n",
       " (424, 0.034789539251215654),\n",
       " (425, 0.03637806299252282),\n",
       " (426, 0.03875190720086737),\n",
       " (427, 0.038628723568890154),\n",
       " (428, 0.03313764861172012),\n",
       " (429, 0.036071053895668316),\n",
       " (430, 0.03361796327862107),\n",
       " (431, 0.037985910475761796),\n",
       " (432, 0.033524093930884846),\n",
       " (433, 0.03276160902239349),\n",
       " (434, 0.03660295296734),\n",
       " (435, 0.037036226390111615),\n",
       " (436, 0.037456606214683656),\n",
       " (437, 0.03725939442729787),\n",
       " (438, 0.03889469515831957),\n",
       " (439, 0.038063435178945705),\n",
       " (440, 0.036906088559481326),\n",
       " (441, 0.03873459910239301),\n",
       " (442, 0.037483729596391915),\n",
       " (443, 0.03475054539306819),\n",
       " (444, 0.03832598285823566),\n",
       " (445, 0.035162670986866484),\n",
       " (446, 0.037301744010898724),\n",
       " (447, 0.03814059778503667),\n",
       " (448, 0.03905520074560369),\n",
       " (449, 0.03553973342520323),\n",
       " (450, 0.03447947247765883),\n",
       " (451, 0.03442787287499093),\n",
       " (452, 0.0349041677730234),\n",
       " (453, 0.03518550920175641),\n",
       " (454, 0.039498001096418565),\n",
       " (455, 0.039589053026702514),\n",
       " (456, 0.03959311117150656),\n",
       " (457, 0.038162974446366446),\n",
       " (458, 0.03845243522540239),\n",
       " (459, 0.03578738030148179),\n",
       " (460, 0.03913756988240383),\n",
       " (461, 0.0347667688766278),\n",
       " (462, 0.037959447771954466),\n",
       " (463, 0.03821013717699136),\n",
       " (464, 0.03883345376657135),\n",
       " (465, 0.037275675670109844),\n",
       " (466, 0.03518778217982267),\n",
       " (467, 0.036323028001659624),\n",
       " (468, 0.037613589447199794),\n",
       " (469, 0.038728679969933205),\n",
       " (470, 0.038290137621550514),\n",
       " (471, 0.037034421058211864),\n",
       " (472, 0.03874508455239791),\n",
       " (473, 0.04092125645605797),\n",
       " (474, 0.03706051346350186),\n",
       " (475, 0.04007309331419347),\n",
       " (476, 0.0374980125044747),\n",
       " (477, 0.03896416835583738),\n",
       " (478, 0.03627674447696244),\n",
       " (479, 0.03486470044519626),\n",
       " (480, 0.03571438737501667),\n",
       " (481, 0.03601191409635445),\n",
       " (482, 0.034957099509004774),\n",
       " (483, 0.03587323827693965),\n",
       " (484, 0.03465704997812258),\n",
       " (485, 0.03410279475906261),\n",
       " (486, 0.038395761932231785),\n",
       " (487, 0.034606949997776165),\n",
       " (488, 0.03858924093404793),\n",
       " (489, 0.03579889538247976),\n",
       " (490, 0.038252692069687004),\n",
       " (491, 0.035475835224861536),\n",
       " (492, 0.03461167928232596),\n",
       " (493, 0.0377609311873119),\n",
       " (494, 0.03970724997139238),\n",
       " (495, 0.03718012732433725),\n",
       " (496, 0.03554423935707803),\n",
       " (497, 0.03840850955414563),\n",
       " (498, 0.03721287169459455),\n",
       " (499, 0.03891023369756571),\n",
       " (500, 0.036987409076187104),\n",
       " (501, 0.03755470094449078),\n",
       " (502, 0.03825238984213714),\n",
       " (503, 0.037464974394828966),\n",
       " (504, 0.03640899031312445),\n",
       " (505, 0.03682141174130732),\n",
       " (506, 0.037004668264925704),\n",
       " (507, 0.03917388673905),\n",
       " (508, 0.03692600101127952),\n",
       " (509, 0.03673092188197801),\n",
       " (510, 0.03684921863457373),\n",
       " (511, 0.036116370954432826),\n",
       " (512, 0.03538661853049439),\n",
       " (513, 0.039064076851893934),\n",
       " (514, 0.035990931855072635),\n",
       " (515, 0.036880281896215124),\n",
       " (516, 0.0334633726834416),\n",
       " (517, 0.037691540078754526),\n",
       " (518, 0.03707577589935939),\n",
       " (519, 0.0351495617709318),\n",
       " (520, 0.03302122285605111),\n",
       " (521, 0.03295653121655584),\n",
       " (522, 0.038357915733565326),\n",
       " (523, 0.03738369301182179),\n",
       " (524, 0.03774744157476889),\n",
       " (525, 0.03786837001488658),\n",
       " (526, 0.04027066695990643),\n",
       " (527, 0.04065101474538294),\n",
       " (528, 0.03474014128811672),\n",
       " (529, 0.036104955823025836),\n",
       " (530, 0.03237549711794325),\n",
       " (531, 0.03572458379135437),\n",
       " (532, 0.03690780447783451),\n",
       " (533, 0.03885955758600858),\n",
       " (534, 0.04033872457192502),\n",
       " (535, 0.03940269836127266),\n",
       " (536, 0.03706160254987203),\n",
       " (537, 0.03707678692060805),\n",
       " (538, 0.038216526582830215),\n",
       " (539, 0.036646707271620005),\n",
       " (540, 0.03633587889423967),\n",
       " (541, 0.037605995516931306),\n",
       " (542, 0.037322896183398666),\n",
       " (543, 0.03539189226099491),\n",
       " (544, 0.03779591344757881),\n",
       " (545, 0.03448193090501776),\n",
       " (546, 0.037469959353353025),\n",
       " (547, 0.0349781871537182),\n",
       " (548, 0.03360089066099351),\n",
       " (549, 0.03585205712658125),\n",
       " (550, 0.03956377650689309),\n",
       " (551, 0.03898755020261027),\n",
       " (552, 0.03490784050492935),\n",
       " (553, 0.033339974566849985),\n",
       " (554, 0.033539836537949536),\n",
       " (555, 0.038147890567742045),\n",
       " (556, 0.03847563179266986),\n",
       " (557, 0.03299262701342071),\n",
       " (558, 0.03803102796057211),\n",
       " (559, 0.037466343768328476),\n",
       " (560, 0.0368272851623441),\n",
       " (561, 0.03750732025173895),\n",
       " (562, 0.03681513755927087),\n",
       " (563, 0.03531862245321562),\n",
       " (564, 0.03481041568265133),\n",
       " (565, 0.03662555558875766),\n",
       " (566, 0.034740159676335765),\n",
       " (567, 0.037209186026510084),\n",
       " (568, 0.037098194563763405),\n",
       " (569, 0.03531167383747923),\n",
       " (570, 0.03460400086447598),\n",
       " (571, 0.03513074300533326),\n",
       " (572, 0.037930494339424387),\n",
       " (573, 0.03667337038520931),\n",
       " (574, 0.03661611390536799),\n",
       " (575, 0.03583160979060955),\n",
       " (576, 0.03398804870728468),\n",
       " (577, 0.033629055083661576),\n",
       " (578, 0.038166445614824435),\n",
       " (579, 0.0384728277897534),\n",
       " (580, 0.03872609950367207),\n",
       " (581, 0.03808941879878283),\n",
       " (582, 0.03898683614793585),\n",
       " (583, 0.03762779412304059),\n",
       " (584, 0.0337501307728054),\n",
       " (585, 0.03841343762348643),\n",
       " (586, 0.03545401446776911),\n",
       " (587, 0.03402317042810032),\n",
       " (588, 0.0368686530510613),\n",
       " (589, 0.03567901410671532),\n",
       " (590, 0.03859373578629224),\n",
       " (591, 0.03647048865829678),\n",
       " (592, 0.03808369932595809),\n",
       " (593, 0.03614728990408797),\n",
       " (594, 0.03473659432286624),\n",
       " (595, 0.0350556643092362),\n",
       " (596, 0.03618195436486135),\n",
       " (597, 0.03659951840508472),\n",
       " (598, 0.037184451056059366),\n",
       " (599, 0.03805632672670627),\n",
       " (600, 0.037320375503777446),\n",
       " (601, 0.03511869721329063),\n",
       " (602, 0.04025896423590843),\n",
       " (603, 0.03894935075546216),\n",
       " (604, 0.03961124508599982),\n",
       " (605, 0.039489452620633844),\n",
       " (606, 0.037050974999188746),\n",
       " (607, 0.037626759690585515),\n",
       " (608, 0.0370180345458601),\n",
       " (609, 0.03845341043024009),\n",
       " (610, 0.03664222763522031),\n",
       " (611, 0.03378118258746206),\n",
       " (612, 0.037809975934335605),\n",
       " (613, 0.03726798833297713),\n",
       " (614, 0.03889864527948938),\n",
       " (615, 0.03960169861946466),\n",
       " (616, 0.036241053683539616),\n",
       " (617, 0.03763500185767745),\n",
       " (618, 0.037525981770086626),\n",
       " (619, 0.03661524025612899),\n",
       " (620, 0.03843300070681966),\n",
       " (621, 0.03687825123585716),\n",
       " (622, 0.038990267096643556),\n",
       " (623, 0.039587412867324055),\n",
       " (624, 0.038559332691684205),\n",
       " (625, 0.03799538652697883),\n",
       " (626, 0.034082981486752914),\n",
       " (627, 0.037428495894048126),\n",
       " (628, 0.03624584288592131),\n",
       " (629, 0.03554469890891465),\n",
       " (630, 0.03482832255340036),\n",
       " (631, 0.03743126748614616),\n",
       " (632, 0.03815770787563014),\n",
       " (633, 0.038121253900438516),\n",
       " (634, 0.0331936376172214),\n",
       " (635, 0.03278717457916911),\n",
       " (636, 0.03893029554239409),\n",
       " (637, 0.038844770344421305),\n",
       " (638, 0.03786027970708197),\n",
       " (639, 0.039940845536948155),\n",
       " (640, 0.039827559330337964),\n",
       " (641, 0.036561150535216844),\n",
       " (642, 0.03638511657371762),\n",
       " (643, 0.03974620552453662),\n",
       " (644, 0.03658092788449953),\n",
       " (645, 0.039953452754909365),\n",
       " (646, 0.03772919280289588),\n",
       " (647, 0.03894895702969245),\n",
       " (648, 0.03632484639597658),\n",
       " (649, 0.03621958174547199),\n",
       " (650, 0.0363307258926747),\n",
       " (651, 0.03811578037556093),\n",
       " (652, 0.03903423578049096),\n",
       " (653, 0.037525692895396444),\n",
       " (654, 0.04039152176126486),\n",
       " (655, 0.03740680832607239),\n",
       " (656, 0.03489620845587107),\n",
       " (657, 0.038813665172479105),\n",
       " (658, 0.037975518515123964),\n",
       " (659, 0.03379354934849661),\n",
       " (660, 0.03449775061557699),\n",
       " (661, 0.03982237385403379),\n",
       " (662, 0.03831280268019478),\n",
       " (663, 0.03345812734087605),\n",
       " (664, 0.036450139505097644),\n",
       " (665, 0.03617638503703028),\n",
       " (666, 0.037144991927349456),\n",
       " (667, 0.036169600009195786),\n",
       " (668, 0.03803564015863617),\n",
       " (669, 0.036042382907707796),\n",
       " (670, 0.03739581365701101),\n",
       " (671, 0.03692123072430947),\n",
       " (672, 0.03579385470369668),\n",
       " (673, 0.03806322318872429),\n",
       " (674, 0.037421826360039046),\n",
       " (675, 0.034851840009465625),\n",
       " (676, 0.036096474612443134),\n",
       " (677, 0.03403233395432509),\n",
       " (678, 0.03922695990997512),\n",
       " (679, 0.036881989381739255),\n",
       " (680, 0.03698161496744351),\n",
       " (681, 0.034712655996299094),\n",
       " (682, 0.034868746161380984),\n",
       " (683, 0.03851731233083324),\n",
       " (684, 0.03854972988624564),\n",
       " (685, 0.03617547309412847),\n",
       " (686, 0.03529922540787525),\n",
       " (687, 0.03648451962151329),\n",
       " (688, 0.03281424364249398),\n",
       " (689, 0.03299321905036029),\n",
       " (690, 0.037295872365524146),\n",
       " (691, 0.039896275536895655),\n",
       " (692, 0.037294241929249565),\n",
       " (693, 0.03819600788743899),\n",
       " (694, 0.03573493384132237),\n",
       " (695, 0.03740872033616219),\n",
       " (696, 0.0365961334510388),\n",
       " (697, 0.03881577510180439),\n",
       " (698, 0.03658895716697924),\n",
       " (699, 0.03592174749183478),\n",
       " (700, 0.035046338923614966),\n",
       " (701, 0.03882472703873708),\n",
       " (702, 0.03782167454612999),\n",
       " (703, 0.03536392594999876),\n",
       " (704, 0.03601583651284241),\n",
       " (705, 0.035878894331844775),\n",
       " (706, 0.03572953301272655),\n",
       " (707, 0.03401866440207561),\n",
       " (708, 0.035318443826159375),\n",
       " (709, 0.03528773670043518),\n",
       " (710, 0.031810741239576154),\n",
       " (711, 0.037090901976301494),\n",
       " (712, 0.03689234435128212),\n",
       " (713, 0.03716418547776892),\n",
       " (714, 0.03680870617637292),\n",
       " (715, 0.038898887959254606),\n",
       " (716, 0.03699679133846152),\n",
       " (717, 0.03664320297594084),\n",
       " (718, 0.03457711054164781),\n",
       " (719, 0.03504051655313015),\n",
       " (720, 0.038743787428390616),\n",
       " (721, 0.03738287716589364),\n",
       " (722, 0.03772098948869133),\n",
       " (723, 0.03844555516555432),\n",
       " (724, 0.03381683829289911),\n",
       " (725, 0.03271512433008126),\n",
       " (726, 0.040047574818538545),\n",
       " (727, 0.04039271637902198),\n",
       " (728, 0.036862664157323914),\n",
       " (729, 0.035514998924749075),\n",
       " (730, 0.03799759617091113),\n",
       " (731, 0.04019462793748589),\n",
       " (732, 0.03581162666856073),\n",
       " (733, 0.03585021690425816),\n",
       " (734, 0.036385762880137035),\n",
       " (735, 0.036413850777400766),\n",
       " (736, 0.034791616266784724),\n",
       " (737, 0.03798295737079737),\n",
       " (738, 0.03640039244115069),\n",
       " (739, 0.036026416853122356),\n",
       " (740, 0.03819831847643373),\n",
       " (741, 0.035702931252078976),\n",
       " (742, 0.03886813474726536),\n",
       " (743, 0.036266427601346844),\n",
       " (744, 0.03834573714385265),\n",
       " (745, 0.03526829316697862),\n",
       " (746, 0.03591006988540559),\n",
       " (747, 0.03463238983875761),\n",
       " (748, 0.031997702947194395),\n",
       " (749, 0.03561679900298705),\n",
       " (750, 0.03855326174414115),\n",
       " (751, 0.03782392333218862),\n",
       " (752, 0.03981379202790218),\n",
       " (753, 0.03957190289413161),\n",
       " (754, 0.035823070310985354),\n",
       " (755, 0.036243381484103325),\n",
       " (756, 0.03758687386938842),\n",
       " (757, 0.0378278582851942),\n",
       " (758, 0.037163558343630845),\n",
       " (759, 0.038459800009442824),\n",
       " (760, 0.03709669144675189),\n",
       " (761, 0.04079153435724106),\n",
       " (762, 0.0388907212545733),\n",
       " (763, 0.03847674339831824),\n",
       " (764, 0.035527186380652005),\n",
       " (765, 0.03600480352936),\n",
       " (766, 0.03713831712620133),\n",
       " (767, 0.04069932914222787),\n",
       " (768, 0.038091602824775445),\n",
       " (769, 0.03940370686422444),\n",
       " (770, 0.03699327782593146),\n",
       " (771, 0.03853437502462761),\n",
       " (772, 0.03940883730505788),\n",
       " (773, 0.03995605161489132),\n",
       " (774, 0.03709581752561159),\n",
       " (775, 0.038095035539114815),\n",
       " (776, 0.03593023156133781),\n",
       " (777, 0.03664558081778009),\n",
       " (778, 0.03476324006155509),\n",
       " (779, 0.03776543412178524),\n",
       " (780, 0.03437666320050549),\n",
       " (781, 0.03478791114500238),\n",
       " (782, 0.03837235356123129),\n",
       " (783, 0.03638326184271422),\n",
       " (784, 0.03965891913021209),\n",
       " (785, 0.036375233485271545),\n",
       " (786, 0.03521876276919572),\n",
       " (787, 0.03853892666491903),\n",
       " (788, 0.03569189725275986),\n",
       " (789, 0.03863534535846998),\n",
       " (790, 0.0336786875696227),\n",
       " (791, 0.03722653522605384),\n",
       " (792, 0.0358889931181728),\n",
       " (793, 0.0377825388951405),\n",
       " (794, 0.03470778709191972),\n",
       " (795, 0.03360678303468955),\n",
       " (796, 0.04009629906751214),\n",
       " (797, 0.037409844831669675),\n",
       " (798, 0.040004114632586366),\n",
       " (799, 0.03967373071528858),\n",
       " (800, 0.03547463053462467),\n",
       " (801, 0.03542100726256568),\n",
       " (802, 0.03415266380063041),\n",
       " (803, 0.037043635147872796),\n",
       " (804, 0.034013909125213376),\n",
       " (805, 0.03747335766088396),\n",
       " (806, 0.03465823809388461),\n",
       " (807, 0.03701271050015786),\n",
       " (808, 0.037155649887308156),\n",
       " (809, 0.03633368336628037),\n",
       " (810, 0.036625996625286726),\n",
       " (811, 0.035831133076595995),\n",
       " (812, 0.033721849039890395),\n",
       " (813, 0.03329152186358455),\n",
       " (814, 0.0359683507960293),\n",
       " (815, 0.034831427662985394),\n",
       " (816, 0.038998756630423716),\n",
       " (817, 0.037452344412719725),\n",
       " (818, 0.03597463050237755),\n",
       " (819, 0.03833104589100504),\n",
       " (820, 0.038959990459735146),\n",
       " (821, 0.034959076687173005),\n",
       " (822, 0.0397063726988646),\n",
       " (823, 0.0380073883037662),\n",
       " (824, 0.03748295289293709),\n",
       " (825, 0.03875369830931302),\n",
       " (826, 0.039139108689418445),\n",
       " (827, 0.03508830865354143),\n",
       " (828, 0.03487247354835782),\n",
       " (829, 0.03633481475280289),\n",
       " (830, 0.038757860777358294),\n",
       " (831, 0.03584926986941169),\n",
       " (832, 0.03570968696651244),\n",
       " (833, 0.036380744821658526),\n",
       " (834, 0.037433301676074275),\n",
       " (835, 0.03580586339031844),\n",
       " (836, 0.037493338662057986),\n",
       " (837, 0.0384408113255319),\n",
       " (838, 0.035087163035902556),\n",
       " (839, 0.036784389890782226),\n",
       " (840, 0.038238557099144026),\n",
       " (841, 0.04050434259983452),\n",
       " (842, 0.040508758567589995),\n",
       " (843, 0.036626142380664826),\n",
       " (844, 0.03521020336647398),\n",
       " (845, 0.036845128484142356),\n",
       " (846, 0.0360869854974156),\n",
       " (847, 0.03885073964241073),\n",
       " (848, 0.03783388486046029),\n",
       " (849, 0.03925633441296053),\n",
       " (850, 0.03655507028202144),\n",
       " (851, 0.03417548367395963),\n",
       " (852, 0.03918456107769291),\n",
       " (853, 0.034301925380914935),\n",
       " (854, 0.03683254681389026),\n",
       " (855, 0.03681357785488326),\n",
       " (856, 0.03396849771484294),\n",
       " (857, 0.03727133022607061),\n",
       " (858, 0.03702120848729458),\n",
       " (859, 0.03622423061694702),\n",
       " (860, 0.03295759687660203),\n",
       " (861, 0.03447311157002756),\n",
       " (862, 0.03631993004378642),\n",
       " (863, 0.03304086749152734),\n",
       " (864, 0.03602724620568581),\n",
       " (865, 0.036272288742645285),\n",
       " (866, 0.03850328210480377),\n",
       " (867, 0.03656122010039122),\n",
       " (868, 0.03964645597095155),\n",
       " (869, 0.037970031588643406),\n",
       " (870, 0.0352105327459895),\n",
       " (871, 0.038103564292046005),\n",
       " (872, 0.034866010055152614),\n",
       " (873, 0.03591793736702268),\n",
       " (874, 0.0379250949037664),\n",
       " (875, 0.03871361129067885),\n",
       " (876, 0.034635061021294156),\n",
       " (877, 0.03321050165722785),\n",
       " (878, 0.03744440944048906),\n",
       " (879, 0.03896688522429852),\n",
       " (880, 0.03384588187243573),\n",
       " (881, 0.03363965370351344),\n",
       " (882, 0.03653449605275585),\n",
       " (883, 0.035910043069029285),\n",
       " (884, 0.03806417022336017),\n",
       " (885, 0.03745171797767196),\n",
       " (886, 0.03929071119445652),\n",
       " (887, 0.03947824170976481),\n",
       " (888, 0.03574603662474979),\n",
       " (889, 0.035785200687627326),\n",
       " (890, 0.0346082631719738),\n",
       " (891, 0.03837316594115815),\n",
       " (892, 0.03753479776576081),\n",
       " (893, 0.03756593844071174),\n",
       " (894, 0.036991414635540516),\n",
       " (895, 0.037620416036841194),\n",
       " (896, 0.0381348785721555),\n",
       " (897, 0.03729586799029123),\n",
       " (898, 0.040712979077498296),\n",
       " (899, 0.035916173345073944),\n",
       " (900, 0.03467380862802699),\n",
       " (901, 0.036525407835717616),\n",
       " (902, 0.03726220650943163),\n",
       " (903, 0.0400805833381579),\n",
       " (904, 0.035669204988948065),\n",
       " (905, 0.035136126421714105),\n",
       " (906, 0.03927447229456956),\n",
       " (907, 0.03833293947967929),\n",
       " (908, 0.03926036759207944),\n",
       " (909, 0.035857579551614414),\n",
       " (910, 0.03486077253675188),\n",
       " (911, 0.03825570232400382),\n",
       " (912, 0.03885910072799926),\n",
       " (913, 0.03595902217010819),\n",
       " (914, 0.037723972155380515),\n",
       " (915, 0.034769774373730196),\n",
       " (916, 0.035522844384220306),\n",
       " (917, 0.036858403328065455),\n",
       " (918, 0.035990601836762955),\n",
       " (919, 0.038474418386910916),\n",
       " (920, 0.03904491719877539),\n",
       " (921, 0.03541921335078389),\n",
       " (922, 0.03301053577107321),\n",
       " (923, 0.03310690331773163),\n",
       " (924, 0.03828664806428822),\n",
       " (925, 0.035163064227882485),\n",
       " (926, 0.03909144199463122),\n",
       " (927, 0.036993248585239426),\n",
       " (928, 0.03699271286582633),\n",
       " (929, 0.03966947550403498),\n",
       " (930, 0.03614065530902028),\n",
       " (931, 0.0368840949839696),\n",
       " (932, 0.03568619753629727),\n",
       " (933, 0.03774717243308499),\n",
       " (934, 0.034500683229669434),\n",
       " (935, 0.03526905345267708),\n",
       " (936, 0.03509803613008241),\n",
       " (937, 0.03570285407222637),\n",
       " (938, 0.034168492976196785),\n",
       " (939, 0.03716233108863206),\n",
       " (940, 0.035435738406199915),\n",
       " (941, 0.03865896909228479),\n",
       " (942, 0.036197245541181),\n",
       " (943, 0.03720162884520415),\n",
       " (944, 0.03547167981119795),\n",
       " (945, 0.03714470993367537),\n",
       " (946, 0.03752750798190512),\n",
       " (947, 0.03632281948018698),\n",
       " (948, 0.037200914568165926),\n",
       " (949, 0.03868661338375902),\n",
       " (950, 0.0365202617167783),\n",
       " (951, 0.03309135201506416),\n",
       " (952, 0.03831881991200813),\n",
       " (953, 0.03561061589682184),\n",
       " (954, 0.039051964972216545),\n",
       " (955, 0.0366256132981286),\n",
       " (956, 0.03848405030891188),\n",
       " (957, 0.03751887140251097),\n",
       " (958, 0.035527505856172724),\n",
       " (959, 0.0390967404600364),\n",
       " (960, 0.03633415030603538),\n",
       " (961, 0.03669431529562169),\n",
       " (962, 0.03689696371655221),\n",
       " (963, 0.036831670728307654),\n",
       " (964, 0.03544228503720643),\n",
       " (965, 0.03444120161998716),\n",
       " (966, 0.03627612190531305),\n",
       " (967, 0.03728076115523936),\n",
       " (968, 0.03487723307463406),\n",
       " (969, 0.037549016311838115),\n",
       " (970, 0.038273446201654),\n",
       " (971, 0.0406969421725546),\n",
       " (972, 0.03928898406653359),\n",
       " (973, 0.03723964590825662),\n",
       " (974, 0.037257952997606474),\n",
       " (975, 0.038782066239624866),\n",
       " (976, 0.03610490420250595),\n",
       " (977, 0.036470696827316386),\n",
       " (978, 0.036588723502708295),\n",
       " (979, 0.03643335157071118),\n",
       " (980, 0.03582496501637365),\n",
       " (981, 0.03655764869298232),\n",
       " (982, 0.038640598889961124),\n",
       " (983, 0.03422860761792169),\n",
       " (984, 0.033535801357395145),\n",
       " (985, 0.034972170598272116),\n",
       " (986, 0.034519463115586554),\n",
       " (987, 0.034417566662877805),\n",
       " (988, 0.03378164121004335),\n",
       " (989, 0.03376356075695426),\n",
       " (990, 0.034202531293209215),\n",
       " (991, 0.0372170408229339),\n",
       " (992, 0.036462110311336476),\n",
       " (993, 0.037476992850602624),\n",
       " (994, 0.03535888456265248),\n",
       " (995, 0.033630065377722805),\n",
       " (996, 0.03321127427113891),\n",
       " (997, 0.03574022821904463),\n",
       " (998, 0.03281915114945409),\n",
       " (999, 0.039579290101155865)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated P - user latent features matrix\n",
      "[[ 1.35776956 -0.45330127]\n",
      " [ 1.01576292 -0.35030624]\n",
      " [-1.42531773  0.2796585 ]\n",
      " [-1.07189331  0.21264541]\n",
      " [-0.99190847 -0.27190721]]\n",
      "\n",
      "updated Q - item latent features matrix\n",
      "[[ 1.34705803 -0.38076503]\n",
      " [ 0.7820832   0.17359821]\n",
      " [-1.02099192 -0.41646427]\n",
      " [-1.3208605   0.32690635]]\n"
     ]
    }
   ],
   "source": [
    "# See how our latent feature matrices P and Q have been updated\n",
    "print('updated P - user latent features matrix')\n",
    "print(latent_ft_mats[0])\n",
    "print()\n",
    "print('updated Q - item latent features matrix')\n",
    "print(latent_ft_mats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated user biases: [ 0.1229443  -0.36421252  0.17645183 -0.3230978  -0.05691351]\n",
      "updated item biases: [ 0.0887355  -0.87447299  1.14972717  0.06107305]\n"
     ]
    }
   ],
   "source": [
    "# See how our biases have been updated\n",
    "print('updated user biases:', biases[0])\n",
    "print('updated item biases:', biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.98250623, 3.00089856, 2.84441428, 1.01163688],\n",
       "       [3.99542972, 2.26414385, 2.66354972, 1.00989284],\n",
       "       [1.00794822, 1.00504077, 5.43417988, 4.98082369],\n",
       "       [1.00999804, 0.77026509, 4.60169533, 3.99254269],\n",
       "       [1.56842724, 1.01488671, 4.98801459, 3.99467483]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See final matrix\n",
    "print('Final Matrix')\n",
    "final_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Matrix Factorization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MF():\n",
    "\n",
    "    def __init__(self, R, K, alpha, beta, iterations):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty\n",
    "        entries in a matrix.\n",
    "        \n",
    "        Arguments:\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - K (int)       : number of latent dimensions\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        \"\"\"\n",
    "\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def train(self):\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "\n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "\n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            mse = self.mse()\n",
    "            training_process.append((i, mse))\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "\n",
    "        return training_process\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Compute prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "\n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Compute the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame([[5, 3, 0, 1],\n",
    "                        [4, 0, 0, 1],\n",
    "                        [1, 1, 0, 5],\n",
    "                        [1, 0, 0, 4],\n",
    "                        [0, 1, 5, 4]],\n",
    "                       index=['U1','U2','U3','U4','U5'],\n",
    "                       columns=['D1','D2','D3','D4'])\n",
    "\n",
    "# Define R\n",
    "R = ratings.values\n",
    "\n",
    "# Initialize the matrix factorization calss\n",
    "mf = MF(R, K=2, alpha=0.1, beta=0.01, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; error = 0.4823\n",
      "Iteration: 20 ; error = 0.1155\n",
      "Iteration: 30 ; error = 0.0459\n",
      "Iteration: 40 ; error = 0.0396\n",
      "Iteration: 50 ; error = 0.0386\n",
      "Iteration: 60 ; error = 0.0365\n",
      "Iteration: 70 ; error = 0.0371\n",
      "Iteration: 80 ; error = 0.0388\n",
      "Iteration: 90 ; error = 0.0399\n",
      "Iteration: 100 ; error = 0.0372\n",
      "Iteration: 110 ; error = 0.0404\n",
      "Iteration: 120 ; error = 0.0366\n",
      "Iteration: 130 ; error = 0.0356\n",
      "Iteration: 140 ; error = 0.0400\n",
      "Iteration: 150 ; error = 0.0388\n",
      "Iteration: 160 ; error = 0.0355\n",
      "Iteration: 170 ; error = 0.0358\n",
      "Iteration: 180 ; error = 0.0390\n",
      "Iteration: 190 ; error = 0.0399\n",
      "Iteration: 200 ; error = 0.0389\n",
      "Iteration: 210 ; error = 0.0372\n",
      "Iteration: 220 ; error = 0.0357\n",
      "Iteration: 230 ; error = 0.0384\n",
      "Iteration: 240 ; error = 0.0340\n",
      "Iteration: 250 ; error = 0.0355\n",
      "Iteration: 260 ; error = 0.0367\n",
      "Iteration: 270 ; error = 0.0404\n",
      "Iteration: 280 ; error = 0.0365\n",
      "Iteration: 290 ; error = 0.0374\n",
      "Iteration: 300 ; error = 0.0393\n",
      "Iteration: 310 ; error = 0.0374\n",
      "Iteration: 320 ; error = 0.0400\n",
      "Iteration: 330 ; error = 0.0394\n",
      "Iteration: 340 ; error = 0.0347\n",
      "Iteration: 350 ; error = 0.0349\n",
      "Iteration: 360 ; error = 0.0369\n",
      "Iteration: 370 ; error = 0.0394\n",
      "Iteration: 380 ; error = 0.0377\n",
      "Iteration: 390 ; error = 0.0382\n",
      "Iteration: 400 ; error = 0.0379\n",
      "Iteration: 410 ; error = 0.0366\n",
      "Iteration: 420 ; error = 0.0391\n",
      "Iteration: 430 ; error = 0.0384\n",
      "Iteration: 440 ; error = 0.0406\n",
      "Iteration: 450 ; error = 0.0386\n",
      "Iteration: 460 ; error = 0.0390\n",
      "Iteration: 470 ; error = 0.0355\n",
      "Iteration: 480 ; error = 0.0384\n",
      "Iteration: 490 ; error = 0.0402\n",
      "Iteration: 500 ; error = 0.0361\n",
      "Iteration: 510 ; error = 0.0377\n",
      "Iteration: 520 ; error = 0.0353\n",
      "Iteration: 530 ; error = 0.0370\n",
      "Iteration: 540 ; error = 0.0384\n",
      "Iteration: 550 ; error = 0.0378\n",
      "Iteration: 560 ; error = 0.0338\n",
      "Iteration: 570 ; error = 0.0375\n",
      "Iteration: 580 ; error = 0.0356\n",
      "Iteration: 590 ; error = 0.0394\n",
      "Iteration: 600 ; error = 0.0371\n",
      "Iteration: 610 ; error = 0.0392\n",
      "Iteration: 620 ; error = 0.0376\n",
      "Iteration: 630 ; error = 0.0389\n",
      "Iteration: 640 ; error = 0.0372\n",
      "Iteration: 650 ; error = 0.0374\n",
      "Iteration: 660 ; error = 0.0359\n",
      "Iteration: 670 ; error = 0.0360\n",
      "Iteration: 680 ; error = 0.0393\n",
      "Iteration: 690 ; error = 0.0391\n",
      "Iteration: 700 ; error = 0.0345\n",
      "Iteration: 710 ; error = 0.0340\n",
      "Iteration: 720 ; error = 0.0381\n",
      "Iteration: 730 ; error = 0.0389\n",
      "Iteration: 740 ; error = 0.0383\n",
      "Iteration: 750 ; error = 0.0407\n",
      "Iteration: 760 ; error = 0.0344\n",
      "Iteration: 770 ; error = 0.0355\n",
      "Iteration: 780 ; error = 0.0408\n",
      "Iteration: 790 ; error = 0.0392\n",
      "Iteration: 800 ; error = 0.0387\n",
      "Iteration: 810 ; error = 0.0347\n",
      "Iteration: 820 ; error = 0.0357\n",
      "Iteration: 830 ; error = 0.0383\n",
      "Iteration: 840 ; error = 0.0369\n",
      "Iteration: 850 ; error = 0.0397\n",
      "Iteration: 860 ; error = 0.0376\n",
      "Iteration: 870 ; error = 0.0396\n",
      "Iteration: 880 ; error = 0.0402\n",
      "Iteration: 890 ; error = 0.0392\n",
      "Iteration: 900 ; error = 0.0367\n",
      "Iteration: 910 ; error = 0.0370\n",
      "Iteration: 920 ; error = 0.0369\n",
      "Iteration: 930 ; error = 0.0396\n",
      "Iteration: 940 ; error = 0.0366\n",
      "Iteration: 950 ; error = 0.0370\n",
      "Iteration: 960 ; error = 0.0352\n",
      "Iteration: 970 ; error = 0.0373\n",
      "Iteration: 980 ; error = 0.0377\n",
      "Iteration: 990 ; error = 0.0372\n",
      "Iteration: 1000 ; error = 0.0393\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "training_procc = mf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5.842055996781901),\n",
       " (1, 5.532194253061392),\n",
       " (2, 5.111687202224677),\n",
       " (3, 4.425241314646567),\n",
       " (4, 3.31694638426138),\n",
       " (5, 2.071262096368515),\n",
       " (6, 1.2139332064395731),\n",
       " (7, 0.8121552093242488),\n",
       " (8, 0.6024508320263808),\n",
       " (9, 0.4823053884330457),\n",
       " (10, 0.3997455561075574),\n",
       " (11, 0.340836959253712),\n",
       " (12, 0.2916102203172867),\n",
       " (13, 0.24972136773010975),\n",
       " (14, 0.22210346102425235),\n",
       " (15, 0.1897232724065017),\n",
       " (16, 0.17154307578497605),\n",
       " (17, 0.15597587559793633),\n",
       " (18, 0.12892681292752298),\n",
       " (19, 0.11547098092787549),\n",
       " (20, 0.10482865292566174),\n",
       " (21, 0.09272199213934385),\n",
       " (22, 0.084290100184207),\n",
       " (23, 0.07320989103187436),\n",
       " (24, 0.07335568289435004),\n",
       " (25, 0.06384177830143835),\n",
       " (26, 0.056794122396598846),\n",
       " (27, 0.05319766504907028),\n",
       " (28, 0.05067649813610341),\n",
       " (29, 0.045885378660585674),\n",
       " (30, 0.0451302492018159),\n",
       " (31, 0.04488865443589238),\n",
       " (32, 0.04463818289596638),\n",
       " (33, 0.040975328894948274),\n",
       " (34, 0.040451151965941255),\n",
       " (35, 0.04062318921782814),\n",
       " (36, 0.04124295859242883),\n",
       " (37, 0.04034220299404753),\n",
       " (38, 0.03943751816145199),\n",
       " (39, 0.03963827049587721),\n",
       " (40, 0.041391443262195786),\n",
       " (41, 0.04177781349611134),\n",
       " (42, 0.04026745673031561),\n",
       " (43, 0.03940333909175391),\n",
       " (44, 0.039746311564878775),\n",
       " (45, 0.0403270721810629),\n",
       " (46, 0.041644163617730684),\n",
       " (47, 0.041490493923991865),\n",
       " (48, 0.04082170751026797),\n",
       " (49, 0.03855441248220882),\n",
       " (50, 0.03622468035066603),\n",
       " (51, 0.03571719358314656),\n",
       " (52, 0.03454527303432169),\n",
       " (53, 0.03663712977168981),\n",
       " (54, 0.0381280183350121),\n",
       " (55, 0.038879562990360786),\n",
       " (56, 0.038544679703793605),\n",
       " (57, 0.03647687606554132),\n",
       " (58, 0.03844787919221272),\n",
       " (59, 0.036505762284763044),\n",
       " (60, 0.03902052893395379),\n",
       " (61, 0.03670630636663581),\n",
       " (62, 0.039815207277364095),\n",
       " (63, 0.039744958142988424),\n",
       " (64, 0.03756827421677508),\n",
       " (65, 0.03689052447846595),\n",
       " (66, 0.03835037468477541),\n",
       " (67, 0.03837648909507552),\n",
       " (68, 0.03917547526315731),\n",
       " (69, 0.03708480419979386),\n",
       " (70, 0.03921643122260277),\n",
       " (71, 0.036912720709147875),\n",
       " (72, 0.039745739610713905),\n",
       " (73, 0.0379677307073734),\n",
       " (74, 0.035997147402773974),\n",
       " (75, 0.037002968365895235),\n",
       " (76, 0.03951859952480887),\n",
       " (77, 0.0398044315279586),\n",
       " (78, 0.04057604403221668),\n",
       " (79, 0.038779817516026285),\n",
       " (80, 0.03489817296841965),\n",
       " (81, 0.03469706928400744),\n",
       " (82, 0.03660470471834142),\n",
       " (83, 0.03528694032251856),\n",
       " (84, 0.03870555964112408),\n",
       " (85, 0.03835652829263505),\n",
       " (86, 0.036221608830798745),\n",
       " (87, 0.03930853033320252),\n",
       " (88, 0.036963917584015994),\n",
       " (89, 0.03992442363160591),\n",
       " (90, 0.0378411664284857),\n",
       " (91, 0.03682644804838955),\n",
       " (92, 0.04164423364577175),\n",
       " (93, 0.03884958484871873),\n",
       " (94, 0.04014337580176303),\n",
       " (95, 0.03785557675445282),\n",
       " (96, 0.035429567712938675),\n",
       " (97, 0.03828866794368044),\n",
       " (98, 0.03815912696941549),\n",
       " (99, 0.03719434707123159),\n",
       " (100, 0.03569596392747254),\n",
       " (101, 0.038870711819739626),\n",
       " (102, 0.03883947902522851),\n",
       " (103, 0.037144761025129105),\n",
       " (104, 0.03959438842600445),\n",
       " (105, 0.03794554230337098),\n",
       " (106, 0.03753010837933938),\n",
       " (107, 0.03973297483514059),\n",
       " (108, 0.03754300294709076),\n",
       " (109, 0.04037400486051006),\n",
       " (110, 0.03768363901449865),\n",
       " (111, 0.036578572219719635),\n",
       " (112, 0.04096506499278242),\n",
       " (113, 0.041200176067178074),\n",
       " (114, 0.04178560181926058),\n",
       " (115, 0.03867417857610837),\n",
       " (116, 0.03792621244500935),\n",
       " (117, 0.03706717923398893),\n",
       " (118, 0.0347695913366513),\n",
       " (119, 0.036558459028939214),\n",
       " (120, 0.03541014587333307),\n",
       " (121, 0.03487964472401747),\n",
       " (122, 0.03900453698817192),\n",
       " (123, 0.03631373589841044),\n",
       " (124, 0.036719654299735595),\n",
       " (125, 0.035496599924848965),\n",
       " (126, 0.03647455594424971),\n",
       " (127, 0.037292538286229374),\n",
       " (128, 0.0359500273840849),\n",
       " (129, 0.03563325634414051),\n",
       " (130, 0.03469011345348609),\n",
       " (131, 0.03513224260033186),\n",
       " (132, 0.03772683102022926),\n",
       " (133, 0.03757068776128428),\n",
       " (134, 0.03893551233165402),\n",
       " (135, 0.03699254269340909),\n",
       " (136, 0.03815196823380797),\n",
       " (137, 0.03627863220190994),\n",
       " (138, 0.037073502908247416),\n",
       " (139, 0.03999955872610162),\n",
       " (140, 0.038962823175090956),\n",
       " (141, 0.04137609077975037),\n",
       " (142, 0.03502713251527019),\n",
       " (143, 0.036911253437192905),\n",
       " (144, 0.03611104870236258),\n",
       " (145, 0.03937443385686036),\n",
       " (146, 0.03948281772521454),\n",
       " (147, 0.03497791782256684),\n",
       " (148, 0.036576268805802205),\n",
       " (149, 0.03882982141837694),\n",
       " (150, 0.03993047127488568),\n",
       " (151, 0.0411024002869442),\n",
       " (152, 0.03919342541098787),\n",
       " (153, 0.03581024373434845),\n",
       " (154, 0.03781849587343455),\n",
       " (155, 0.037723133646944185),\n",
       " (156, 0.036324587692586736),\n",
       " (157, 0.03493906172484685),\n",
       " (158, 0.03750041475146818),\n",
       " (159, 0.03548931221271507),\n",
       " (160, 0.03786691520317592),\n",
       " (161, 0.03428717200236054),\n",
       " (162, 0.03774203177375835),\n",
       " (163, 0.03964238547649316),\n",
       " (164, 0.03831642307479862),\n",
       " (165, 0.035988310048321655),\n",
       " (166, 0.03693776622781902),\n",
       " (167, 0.03896284383334262),\n",
       " (168, 0.039625511924905675),\n",
       " (169, 0.035846552532275366),\n",
       " (170, 0.03565000106457397),\n",
       " (171, 0.04002075973872819),\n",
       " (172, 0.039683522143097),\n",
       " (173, 0.036326045108713116),\n",
       " (174, 0.037943422078466696),\n",
       " (175, 0.038588924823731496),\n",
       " (176, 0.04001877043270577),\n",
       " (177, 0.04010552164725285),\n",
       " (178, 0.0386684475924071),\n",
       " (179, 0.03899844231486102),\n",
       " (180, 0.034541726035644905),\n",
       " (181, 0.033272089200554945),\n",
       " (182, 0.037863581690843855),\n",
       " (183, 0.03633429114044148),\n",
       " (184, 0.03793819574610485),\n",
       " (185, 0.0391587467884808),\n",
       " (186, 0.03650111247250523),\n",
       " (187, 0.039583967502573675),\n",
       " (188, 0.03821281496961478),\n",
       " (189, 0.03987854483980707),\n",
       " (190, 0.03818778968272919),\n",
       " (191, 0.03741606247370288),\n",
       " (192, 0.03546487972544006),\n",
       " (193, 0.03831010606547171),\n",
       " (194, 0.038489720916440005),\n",
       " (195, 0.037932417114828185),\n",
       " (196, 0.03925635041808407),\n",
       " (197, 0.03537055791641749),\n",
       " (198, 0.03751128334863488),\n",
       " (199, 0.03892168978107218),\n",
       " (200, 0.039126904853881934),\n",
       " (201, 0.037940052958086556),\n",
       " (202, 0.037596576290925575),\n",
       " (203, 0.037088966050817254),\n",
       " (204, 0.03770031609395686),\n",
       " (205, 0.03544211718660272),\n",
       " (206, 0.03692710466387759),\n",
       " (207, 0.03838124689076145),\n",
       " (208, 0.03685790447628501),\n",
       " (209, 0.03716825590310544),\n",
       " (210, 0.03512208101798759),\n",
       " (211, 0.0399905241032943),\n",
       " (212, 0.037261289586992125),\n",
       " (213, 0.03380495968820744),\n",
       " (214, 0.039275305802683944),\n",
       " (215, 0.039233901908944116),\n",
       " (216, 0.03785184136979883),\n",
       " (217, 0.0359746244144127),\n",
       " (218, 0.03767626132362084),\n",
       " (219, 0.03570973845368943),\n",
       " (220, 0.039636357734559494),\n",
       " (221, 0.03895523105343735),\n",
       " (222, 0.035793278667572946),\n",
       " (223, 0.036712375856525946),\n",
       " (224, 0.03857296737910455),\n",
       " (225, 0.03928449814563896),\n",
       " (226, 0.03625885331587516),\n",
       " (227, 0.03883329702702955),\n",
       " (228, 0.03745341863449553),\n",
       " (229, 0.03844372826959328),\n",
       " (230, 0.035906682242561126),\n",
       " (231, 0.037610244381280813),\n",
       " (232, 0.03758933600575142),\n",
       " (233, 0.037729864126794944),\n",
       " (234, 0.03516560753635115),\n",
       " (235, 0.03747767800781835),\n",
       " (236, 0.03739752471680348),\n",
       " (237, 0.03942361479831863),\n",
       " (238, 0.03671982542378524),\n",
       " (239, 0.03395462984098665),\n",
       " (240, 0.03715950369826759),\n",
       " (241, 0.03531242212523008),\n",
       " (242, 0.03715534355971772),\n",
       " (243, 0.03814956247598977),\n",
       " (244, 0.035464121909820026),\n",
       " (245, 0.038627813699736765),\n",
       " (246, 0.03644223271738088),\n",
       " (247, 0.0388845066716332),\n",
       " (248, 0.04043241253635671),\n",
       " (249, 0.03545665301137518),\n",
       " (250, 0.038665028381006515),\n",
       " (251, 0.037296042248679384),\n",
       " (252, 0.038104039842192974),\n",
       " (253, 0.03950038204481466),\n",
       " (254, 0.04064659023078555),\n",
       " (255, 0.03822607722125666),\n",
       " (256, 0.040656729548819284),\n",
       " (257, 0.04024952984788621),\n",
       " (258, 0.036602401947357054),\n",
       " (259, 0.036748502248026),\n",
       " (260, 0.03805924324513907),\n",
       " (261, 0.03848765191003595),\n",
       " (262, 0.03825379334462501),\n",
       " (263, 0.03770889720613853),\n",
       " (264, 0.0368132862337456),\n",
       " (265, 0.03746232299725423),\n",
       " (266, 0.039195302613723094),\n",
       " (267, 0.03442834039705623),\n",
       " (268, 0.03713288675305287),\n",
       " (269, 0.04035110349615991),\n",
       " (270, 0.0402208943661214),\n",
       " (271, 0.038027874182161275),\n",
       " (272, 0.0355985808123531),\n",
       " (273, 0.03772610161736275),\n",
       " (274, 0.03925517018134356),\n",
       " (275, 0.035530219630223264),\n",
       " (276, 0.037741841531916655),\n",
       " (277, 0.037145375406556956),\n",
       " (278, 0.0360238388034513),\n",
       " (279, 0.03652293876179923),\n",
       " (280, 0.03962590864353979),\n",
       " (281, 0.03885703356352517),\n",
       " (282, 0.03595855153425187),\n",
       " (283, 0.03504370625013602),\n",
       " (284, 0.033852726140410114),\n",
       " (285, 0.03937148238284449),\n",
       " (286, 0.03556600000086689),\n",
       " (287, 0.036103847284270116),\n",
       " (288, 0.03636087577462516),\n",
       " (289, 0.03739510118007947),\n",
       " (290, 0.03663967211814993),\n",
       " (291, 0.038488807070185184),\n",
       " (292, 0.036589351439249386),\n",
       " (293, 0.03644932972068259),\n",
       " (294, 0.035766219477321864),\n",
       " (295, 0.03738625317495084),\n",
       " (296, 0.038361225815566315),\n",
       " (297, 0.03979128481054542),\n",
       " (298, 0.03801879258381616),\n",
       " (299, 0.039253421821342),\n",
       " (300, 0.04022502950401807),\n",
       " (301, 0.03992323942949964),\n",
       " (302, 0.03703249902362482),\n",
       " (303, 0.03797631023593729),\n",
       " (304, 0.03377065305672467),\n",
       " (305, 0.03618024828590145),\n",
       " (306, 0.03793661995942554),\n",
       " (307, 0.03851212249912385),\n",
       " (308, 0.03833270891494593),\n",
       " (309, 0.037439294026808546),\n",
       " (310, 0.03662893569332049),\n",
       " (311, 0.03781093176794369),\n",
       " (312, 0.037832102724766656),\n",
       " (313, 0.03862166422421703),\n",
       " (314, 0.03609134936516918),\n",
       " (315, 0.03776518775848622),\n",
       " (316, 0.03703573612850556),\n",
       " (317, 0.03924236522063542),\n",
       " (318, 0.04037484345556096),\n",
       " (319, 0.04001763032849437),\n",
       " (320, 0.03524520636054017),\n",
       " (321, 0.0387235879041614),\n",
       " (322, 0.03878692619767594),\n",
       " (323, 0.040440376864000566),\n",
       " (324, 0.03710326729903764),\n",
       " (325, 0.03782857075302719),\n",
       " (326, 0.035648193057030125),\n",
       " (327, 0.03419811326796794),\n",
       " (328, 0.03492028644093741),\n",
       " (329, 0.03944103042577579),\n",
       " (330, 0.03851767810016545),\n",
       " (331, 0.03318608346928771),\n",
       " (332, 0.03714372924333315),\n",
       " (333, 0.03465235260025821),\n",
       " (334, 0.037003301182326864),\n",
       " (335, 0.03687396493061761),\n",
       " (336, 0.034212536016750665),\n",
       " (337, 0.03746537161949696),\n",
       " (338, 0.03515198129382902),\n",
       " (339, 0.034716249532857325),\n",
       " (340, 0.038015162347792085),\n",
       " (341, 0.039246976202661744),\n",
       " (342, 0.035895376492431616),\n",
       " (343, 0.036070046943518536),\n",
       " (344, 0.03670526523523034),\n",
       " (345, 0.03654741443843813),\n",
       " (346, 0.03961436930750608),\n",
       " (347, 0.03877499559145377),\n",
       " (348, 0.03400139116076805),\n",
       " (349, 0.034852030849146134),\n",
       " (350, 0.03645906246934476),\n",
       " (351, 0.038805906018340414),\n",
       " (352, 0.033058329873134555),\n",
       " (353, 0.03250742450917131),\n",
       " (354, 0.039761380522067936),\n",
       " (355, 0.03314571995615661),\n",
       " (356, 0.0348329139049141),\n",
       " (357, 0.03503506385300502),\n",
       " (358, 0.03795383931319441),\n",
       " (359, 0.03692091988541903),\n",
       " (360, 0.03913557744419025),\n",
       " (361, 0.03998889016642126),\n",
       " (362, 0.038770829881513424),\n",
       " (363, 0.03836698918177812),\n",
       " (364, 0.03554942241866339),\n",
       " (365, 0.037983240503519006),\n",
       " (366, 0.03971527729558959),\n",
       " (367, 0.03757821710526505),\n",
       " (368, 0.03857344910704921),\n",
       " (369, 0.03935274624394227),\n",
       " (370, 0.039033361587591825),\n",
       " (371, 0.03608218260067476),\n",
       " (372, 0.03417509091364936),\n",
       " (373, 0.03523544795091315),\n",
       " (374, 0.03750552542639164),\n",
       " (375, 0.04011614223596138),\n",
       " (376, 0.03731719899496818),\n",
       " (377, 0.03620992108041082),\n",
       " (378, 0.037253129164094144),\n",
       " (379, 0.03771407177243758),\n",
       " (380, 0.03806958980805187),\n",
       " (381, 0.039303021693368266),\n",
       " (382, 0.039900813414400604),\n",
       " (383, 0.03651997926277805),\n",
       " (384, 0.03912518291450522),\n",
       " (385, 0.03613668433851359),\n",
       " (386, 0.03807368548207954),\n",
       " (387, 0.034292609491246996),\n",
       " (388, 0.03704053491407855),\n",
       " (389, 0.038222574607672984),\n",
       " (390, 0.036455916508006316),\n",
       " (391, 0.0361572742161308),\n",
       " (392, 0.03816124815267439),\n",
       " (393, 0.0353032071316707),\n",
       " (394, 0.03970884314834512),\n",
       " (395, 0.035357243488217914),\n",
       " (396, 0.03711598435494797),\n",
       " (397, 0.03765851778383703),\n",
       " (398, 0.03854962411763016),\n",
       " (399, 0.03790909799226197),\n",
       " (400, 0.03598513528156757),\n",
       " (401, 0.03598960063746705),\n",
       " (402, 0.03797199493568034),\n",
       " (403, 0.03986105775993976),\n",
       " (404, 0.035726080685225724),\n",
       " (405, 0.03791884526574052),\n",
       " (406, 0.03889494072738871),\n",
       " (407, 0.03979371061377949),\n",
       " (408, 0.03799824812983167),\n",
       " (409, 0.03657029516171449),\n",
       " (410, 0.03495756789244308),\n",
       " (411, 0.03555778802539604),\n",
       " (412, 0.03749178901152887),\n",
       " (413, 0.0401531861117568),\n",
       " (414, 0.03767727431366262),\n",
       " (415, 0.03781906655535256),\n",
       " (416, 0.03529588704665233),\n",
       " (417, 0.03240714429989034),\n",
       " (418, 0.03679625404648215),\n",
       " (419, 0.0390744275971376),\n",
       " (420, 0.03937273132447729),\n",
       " (421, 0.038999593943914716),\n",
       " (422, 0.03464042421692054),\n",
       " (423, 0.03555497783766459),\n",
       " (424, 0.0357569230295384),\n",
       " (425, 0.03856312331292491),\n",
       " (426, 0.03831438036260907),\n",
       " (427, 0.03856953310087389),\n",
       " (428, 0.03467575356838344),\n",
       " (429, 0.038446764746780625),\n",
       " (430, 0.04058529997437379),\n",
       " (431, 0.03518503798857568),\n",
       " (432, 0.03676471534217798),\n",
       " (433, 0.03451929761827602),\n",
       " (434, 0.03610529781074851),\n",
       " (435, 0.03923312543763239),\n",
       " (436, 0.03794997962390213),\n",
       " (437, 0.035746124030086604),\n",
       " (438, 0.03719039427719723),\n",
       " (439, 0.04061858254806289),\n",
       " (440, 0.036582700114811996),\n",
       " (441, 0.03727339108111203),\n",
       " (442, 0.03456205709151145),\n",
       " (443, 0.037343334934209695),\n",
       " (444, 0.03862021760386121),\n",
       " (445, 0.03885268396545307),\n",
       " (446, 0.03698729244451571),\n",
       " (447, 0.034070858870616685),\n",
       " (448, 0.03729355175237209),\n",
       " (449, 0.03858495075425149),\n",
       " (450, 0.036864689147668085),\n",
       " (451, 0.036695447411421986),\n",
       " (452, 0.036054413272122325),\n",
       " (453, 0.035118825751672196),\n",
       " (454, 0.03443679884007309),\n",
       " (455, 0.03868139038274457),\n",
       " (456, 0.03633716947616147),\n",
       " (457, 0.036298296499723964),\n",
       " (458, 0.03683714682800019),\n",
       " (459, 0.03901668859342927),\n",
       " (460, 0.03746765913057259),\n",
       " (461, 0.040859326226577404),\n",
       " (462, 0.0369126869657016),\n",
       " (463, 0.035456796855924),\n",
       " (464, 0.039287835233668855),\n",
       " (465, 0.036600549053103164),\n",
       " (466, 0.03767438646990953),\n",
       " (467, 0.03652557339104343),\n",
       " (468, 0.03694997678699691),\n",
       " (469, 0.03548801418708706),\n",
       " (470, 0.03408789951083545),\n",
       " (471, 0.04002684555948813),\n",
       " (472, 0.03847645839798512),\n",
       " (473, 0.04023875537576557),\n",
       " (474, 0.0389634200742738),\n",
       " (475, 0.03717488110331113),\n",
       " (476, 0.03733316513015017),\n",
       " (477, 0.03753827274263514),\n",
       " (478, 0.03824716943418125),\n",
       " (479, 0.0383988505944494),\n",
       " (480, 0.03986869561988264),\n",
       " (481, 0.03481507993069964),\n",
       " (482, 0.03522785518440943),\n",
       " (483, 0.03562126720551455),\n",
       " (484, 0.03630437988169128),\n",
       " (485, 0.03558730438740789),\n",
       " (486, 0.03796651800184365),\n",
       " (487, 0.03629218528643122),\n",
       " (488, 0.036152932999477666),\n",
       " (489, 0.04015765976258114),\n",
       " (490, 0.03623364001945143),\n",
       " (491, 0.038123634413067174),\n",
       " (492, 0.03977278903085595),\n",
       " (493, 0.04022802806343883),\n",
       " (494, 0.039436705337029906),\n",
       " (495, 0.038565279767869366),\n",
       " (496, 0.03996506437156127),\n",
       " (497, 0.037859774052470305),\n",
       " (498, 0.03519940552481834),\n",
       " (499, 0.03609698958028508),\n",
       " (500, 0.03577745567252738),\n",
       " (501, 0.03841402226487487),\n",
       " (502, 0.040104315087635),\n",
       " (503, 0.03841343992706291),\n",
       " (504, 0.037895575089031755),\n",
       " (505, 0.038099252356571175),\n",
       " (506, 0.03788869147710147),\n",
       " (507, 0.03701793232136623),\n",
       " (508, 0.03651115090898202),\n",
       " (509, 0.03768782427073612),\n",
       " (510, 0.0394310651721875),\n",
       " (511, 0.03730390069514215),\n",
       " (512, 0.037911677401881796),\n",
       " (513, 0.036647124122445626),\n",
       " (514, 0.03677055853980638),\n",
       " (515, 0.036357110051961176),\n",
       " (516, 0.03595534362945243),\n",
       " (517, 0.03465911557353554),\n",
       " (518, 0.035153226695739996),\n",
       " (519, 0.03529301147778887),\n",
       " (520, 0.037528421296509414),\n",
       " (521, 0.03630294469924749),\n",
       " (522, 0.035093189402921575),\n",
       " (523, 0.03910783467368676),\n",
       " (524, 0.03967229236555177),\n",
       " (525, 0.03611428068172829),\n",
       " (526, 0.03893501387116473),\n",
       " (527, 0.03933859131419887),\n",
       " (528, 0.036953223538602704),\n",
       " (529, 0.03695615849319642),\n",
       " (530, 0.03586228023253397),\n",
       " (531, 0.0351933391035096),\n",
       " (532, 0.03963400241973639),\n",
       " (533, 0.03916266792510006),\n",
       " (534, 0.03797122376216801),\n",
       " (535, 0.036715791554527324),\n",
       " (536, 0.03815436764417212),\n",
       " (537, 0.03617289264469939),\n",
       " (538, 0.03865252860641016),\n",
       " (539, 0.038440161287027166),\n",
       " (540, 0.038076641951786526),\n",
       " (541, 0.03818872090640289),\n",
       " (542, 0.03791065783898533),\n",
       " (543, 0.03757882181257242),\n",
       " (544, 0.03799411298529811),\n",
       " (545, 0.03963392976946891),\n",
       " (546, 0.03758430040804629),\n",
       " (547, 0.040443057678698914),\n",
       " (548, 0.03729742222105453),\n",
       " (549, 0.03784596242443315),\n",
       " (550, 0.03674873802197633),\n",
       " (551, 0.033158570305597136),\n",
       " (552, 0.03832372893335764),\n",
       " (553, 0.039019668356656836),\n",
       " (554, 0.03837075661214372),\n",
       " (555, 0.03957316721799946),\n",
       " (556, 0.03944141029192814),\n",
       " (557, 0.03619625617464091),\n",
       " (558, 0.037847720840901385),\n",
       " (559, 0.03375282826464311),\n",
       " (560, 0.0386180296415471),\n",
       " (561, 0.03874278976406948),\n",
       " (562, 0.03773198559206006),\n",
       " (563, 0.037228382698424496),\n",
       " (564, 0.03716613384065093),\n",
       " (565, 0.03722254660352796),\n",
       " (566, 0.03617508562115856),\n",
       " (567, 0.03638052076135806),\n",
       " (568, 0.036265562260873796),\n",
       " (569, 0.03751864940274473),\n",
       " (570, 0.03781330160367455),\n",
       " (571, 0.036920781297226514),\n",
       " (572, 0.03650301410184142),\n",
       " (573, 0.04044009512883879),\n",
       " (574, 0.03588408936263137),\n",
       " (575, 0.03620838877544053),\n",
       " (576, 0.035753247316768974),\n",
       " (577, 0.03684785152944573),\n",
       " (578, 0.03587019258912512),\n",
       " (579, 0.035628081328849134),\n",
       " (580, 0.03752958958468199),\n",
       " (581, 0.03939888841958612),\n",
       " (582, 0.0373611428532275),\n",
       " (583, 0.040078667011164526),\n",
       " (584, 0.039364337451812934),\n",
       " (585, 0.03787217781030328),\n",
       " (586, 0.034290972547772657),\n",
       " (587, 0.034283031020690274),\n",
       " (588, 0.037372006380155795),\n",
       " (589, 0.03942423668243495),\n",
       " (590, 0.03616565649988371),\n",
       " (591, 0.03711701762787365),\n",
       " (592, 0.03405846198153372),\n",
       " (593, 0.039511243182050126),\n",
       " (594, 0.03815348879312801),\n",
       " (595, 0.03468255337075626),\n",
       " (596, 0.036802418219046534),\n",
       " (597, 0.03883927834260769),\n",
       " (598, 0.03706449102293177),\n",
       " (599, 0.037115997341710526),\n",
       " (600, 0.038417951255483244),\n",
       " (601, 0.034173852830017885),\n",
       " (602, 0.039106470588444395),\n",
       " (603, 0.03703510480663092),\n",
       " (604, 0.03869655667701705),\n",
       " (605, 0.03513302076906034),\n",
       " (606, 0.03866283995813409),\n",
       " (607, 0.03675543522433266),\n",
       " (608, 0.036572462257401614),\n",
       " (609, 0.03921431494777408),\n",
       " (610, 0.035690787999811194),\n",
       " (611, 0.03849173625724488),\n",
       " (612, 0.03760564936987012),\n",
       " (613, 0.039347200238657266),\n",
       " (614, 0.03647695112365883),\n",
       " (615, 0.036688360179710014),\n",
       " (616, 0.037389013033770284),\n",
       " (617, 0.03361883970379393),\n",
       " (618, 0.040702227533083175),\n",
       " (619, 0.03764346838579136),\n",
       " (620, 0.035915535858313266),\n",
       " (621, 0.03449959081128013),\n",
       " (622, 0.036244043611445675),\n",
       " (623, 0.035890416522790416),\n",
       " (624, 0.035256744829928216),\n",
       " (625, 0.03656848298496199),\n",
       " (626, 0.03496458926618485),\n",
       " (627, 0.03519181517123707),\n",
       " (628, 0.03903805530814596),\n",
       " (629, 0.038936175344521096),\n",
       " (630, 0.03591803528140453),\n",
       " (631, 0.0369040228190702),\n",
       " (632, 0.03712170400311803),\n",
       " (633, 0.036388060622569794),\n",
       " (634, 0.034348331567951806),\n",
       " (635, 0.03714666144006862),\n",
       " (636, 0.03619212805550488),\n",
       " (637, 0.03517355287700433),\n",
       " (638, 0.036679621900791326),\n",
       " (639, 0.03723060857129476),\n",
       " (640, 0.04058585007506815),\n",
       " (641, 0.03906196407338337),\n",
       " (642, 0.03493028690512673),\n",
       " (643, 0.03462034742035721),\n",
       " (644, 0.03262342816101733),\n",
       " (645, 0.03591821534905053),\n",
       " (646, 0.03848691986952523),\n",
       " (647, 0.04116255195665492),\n",
       " (648, 0.03596246095051258),\n",
       " (649, 0.03741383111489931),\n",
       " (650, 0.03283730942977607),\n",
       " (651, 0.03627185311417007),\n",
       " (652, 0.03787898438155808),\n",
       " (653, 0.03569945828002507),\n",
       " (654, 0.0390176621996229),\n",
       " (655, 0.03548728856810374),\n",
       " (656, 0.036105858818180056),\n",
       " (657, 0.037849182760025996),\n",
       " (658, 0.035553507657127045),\n",
       " (659, 0.035911425916230796),\n",
       " (660, 0.03794859524177335),\n",
       " (661, 0.037094925469662206),\n",
       " (662, 0.03889955904669845),\n",
       " (663, 0.03983652287931248),\n",
       " (664, 0.03769120649444118),\n",
       " (665, 0.03450762414348708),\n",
       " (666, 0.03697048205990746),\n",
       " (667, 0.03787164462234681),\n",
       " (668, 0.03887915273539682),\n",
       " (669, 0.035993204448126516),\n",
       " (670, 0.036563718049651174),\n",
       " (671, 0.03469748351593841),\n",
       " (672, 0.035705945521122585),\n",
       " (673, 0.03592230991998091),\n",
       " (674, 0.03942966710486616),\n",
       " (675, 0.03464663610235129),\n",
       " (676, 0.03912450219665478),\n",
       " (677, 0.03871723297511154),\n",
       " (678, 0.037376886727527314),\n",
       " (679, 0.039255151252878755),\n",
       " (680, 0.03520112186108327),\n",
       " (681, 0.03969907762717745),\n",
       " (682, 0.039615839690954155),\n",
       " (683, 0.03483489235653449),\n",
       " (684, 0.03743844330464755),\n",
       " (685, 0.037645735392366604),\n",
       " (686, 0.038693299018571985),\n",
       " (687, 0.03965108049762708),\n",
       " (688, 0.040565490985771224),\n",
       " (689, 0.03911732269299353),\n",
       " (690, 0.037282131144728003),\n",
       " (691, 0.03635424515498638),\n",
       " (692, 0.03631171242171496),\n",
       " (693, 0.038255653922465646),\n",
       " (694, 0.04012692665112561),\n",
       " (695, 0.03959894631894995),\n",
       " (696, 0.03536596791843526),\n",
       " (697, 0.0393448985477479),\n",
       " (698, 0.036076684126559684),\n",
       " (699, 0.03450443154125754),\n",
       " (700, 0.03864397362286366),\n",
       " (701, 0.04040298463991174),\n",
       " (702, 0.036395950179080504),\n",
       " (703, 0.03611216441593062),\n",
       " (704, 0.037643456360849786),\n",
       " (705, 0.04005737972365755),\n",
       " (706, 0.03830982017877776),\n",
       " (707, 0.034231424350545665),\n",
       " (708, 0.03848225793767382),\n",
       " (709, 0.033999316721835086),\n",
       " (710, 0.03683543196761422),\n",
       " (711, 0.03823578231282915),\n",
       " (712, 0.03560648185751648),\n",
       " (713, 0.03665039715200749),\n",
       " (714, 0.038592288354284944),\n",
       " (715, 0.0385021079581001),\n",
       " (716, 0.03621403966763639),\n",
       " (717, 0.038991589498569706),\n",
       " (718, 0.037552381035344716),\n",
       " (719, 0.03813076001478419),\n",
       " (720, 0.03325941341064559),\n",
       " (721, 0.03791748926778182),\n",
       " (722, 0.03952980405221044),\n",
       " (723, 0.03709515866633658),\n",
       " (724, 0.037415413634651125),\n",
       " (725, 0.03547119856299007),\n",
       " (726, 0.03849937003716776),\n",
       " (727, 0.03767321225158593),\n",
       " (728, 0.041250465543981554),\n",
       " (729, 0.03885800594241494),\n",
       " (730, 0.037647564457789794),\n",
       " (731, 0.04098546007188007),\n",
       " (732, 0.03393096190094147),\n",
       " (733, 0.0389907167492758),\n",
       " (734, 0.0364782411152465),\n",
       " (735, 0.03770577143534133),\n",
       " (736, 0.03718535429502689),\n",
       " (737, 0.039127105985869944),\n",
       " (738, 0.03704460192248576),\n",
       " (739, 0.0382544099169827),\n",
       " (740, 0.0362908538447589),\n",
       " (741, 0.03738637466768697),\n",
       " (742, 0.039228697709836974),\n",
       " (743, 0.03784763438095684),\n",
       " (744, 0.03669093461576735),\n",
       " (745, 0.03945375219305398),\n",
       " (746, 0.03534374032379138),\n",
       " (747, 0.033673372667178875),\n",
       " (748, 0.03861621377875209),\n",
       " (749, 0.040731381493910326),\n",
       " (750, 0.03587536657709924),\n",
       " (751, 0.038234057873091365),\n",
       " (752, 0.03740608801602574),\n",
       " (753, 0.03876417077318419),\n",
       " (754, 0.032967686493038315),\n",
       " (755, 0.03759708657098497),\n",
       " (756, 0.038667212335375255),\n",
       " (757, 0.03477147660490231),\n",
       " (758, 0.03475926124231526),\n",
       " (759, 0.034400700717722804),\n",
       " (760, 0.03581248494395282),\n",
       " (761, 0.035697915022748494),\n",
       " (762, 0.03713727972394373),\n",
       " (763, 0.03888169804046022),\n",
       " (764, 0.03562481671808895),\n",
       " (765, 0.03833635354044244),\n",
       " (766, 0.03794377633194491),\n",
       " (767, 0.039853160156688085),\n",
       " (768, 0.0356723940416936),\n",
       " (769, 0.03553330288436356),\n",
       " (770, 0.03601664014271926),\n",
       " (771, 0.03822470025019365),\n",
       " (772, 0.03741433169397913),\n",
       " (773, 0.03859140702110996),\n",
       " (774, 0.03579134942031303),\n",
       " (775, 0.03955573656663244),\n",
       " (776, 0.03924027880656867),\n",
       " (777, 0.03602149531797064),\n",
       " (778, 0.037804089365828956),\n",
       " (779, 0.040773267573495725),\n",
       " (780, 0.037552682008110815),\n",
       " (781, 0.04010152826603556),\n",
       " (782, 0.03655314632272233),\n",
       " (783, 0.03625742224878685),\n",
       " (784, 0.03718846319337155),\n",
       " (785, 0.038149471962439875),\n",
       " (786, 0.03985625910335853),\n",
       " (787, 0.03821895167375616),\n",
       " (788, 0.03627672029128121),\n",
       " (789, 0.03915323759384832),\n",
       " (790, 0.03595536296705056),\n",
       " (791, 0.037078809397502736),\n",
       " (792, 0.03937787525188455),\n",
       " (793, 0.04148459143014959),\n",
       " (794, 0.03680622798685336),\n",
       " (795, 0.03685763563581698),\n",
       " (796, 0.03897072828623065),\n",
       " (797, 0.03915959842996404),\n",
       " (798, 0.03714445000657569),\n",
       " (799, 0.03871911681253506),\n",
       " (800, 0.039116111661302425),\n",
       " (801, 0.037484099896690264),\n",
       " (802, 0.037816107694077915),\n",
       " (803, 0.03798746630939897),\n",
       " (804, 0.03814369978515766),\n",
       " (805, 0.03749449616867542),\n",
       " (806, 0.037184010677058066),\n",
       " (807, 0.03668569723133551),\n",
       " (808, 0.03416514710920099),\n",
       " (809, 0.034658658842792796),\n",
       " (810, 0.03546810855821901),\n",
       " (811, 0.03358383156710791),\n",
       " (812, 0.03633302639582685),\n",
       " (813, 0.037118120783969095),\n",
       " (814, 0.036030705248261186),\n",
       " (815, 0.03596613836531365),\n",
       " (816, 0.035411466583219836),\n",
       " (817, 0.03954475030222609),\n",
       " (818, 0.03935919500383049),\n",
       " (819, 0.03568069064660177),\n",
       " (820, 0.036819341238313714),\n",
       " (821, 0.0359594298302715),\n",
       " (822, 0.038119612695460714),\n",
       " (823, 0.0393673130752986),\n",
       " (824, 0.03943644727718955),\n",
       " (825, 0.03768432352837542),\n",
       " (826, 0.034902950287378186),\n",
       " (827, 0.03545331890273666),\n",
       " (828, 0.035774688372699336),\n",
       " (829, 0.03829314427042836),\n",
       " (830, 0.03365059098045665),\n",
       " (831, 0.03344538665406568),\n",
       " (832, 0.03685025621855698),\n",
       " (833, 0.03434479230428494),\n",
       " (834, 0.03386356918216055),\n",
       " (835, 0.03834623870010395),\n",
       " (836, 0.037092227927197574),\n",
       " (837, 0.038176952934178124),\n",
       " (838, 0.033568797242046026),\n",
       " (839, 0.036930738922785235),\n",
       " (840, 0.037010865280482315),\n",
       " (841, 0.04024629530659307),\n",
       " (842, 0.03752041813672389),\n",
       " (843, 0.039589430490130195),\n",
       " (844, 0.03885745095986077),\n",
       " (845, 0.035295970332326995),\n",
       " (846, 0.03845192781382753),\n",
       " (847, 0.03773020280174199),\n",
       " (848, 0.03987032822718278),\n",
       " (849, 0.03974075911843771),\n",
       " (850, 0.03846242397638734),\n",
       " (851, 0.03700091509229441),\n",
       " (852, 0.0345858946199501),\n",
       " (853, 0.03319524311298939),\n",
       " (854, 0.03757416085666607),\n",
       " (855, 0.03740885322457503),\n",
       " (856, 0.037469099384353326),\n",
       " (857, 0.03545992418220915),\n",
       " (858, 0.03738963536166849),\n",
       " (859, 0.03758421950248512),\n",
       " (860, 0.03416555330483229),\n",
       " (861, 0.036837645131416845),\n",
       " (862, 0.03500069260045281),\n",
       " (863, 0.040200703044716665),\n",
       " (864, 0.03888549092883862),\n",
       " (865, 0.03620625839116914),\n",
       " (866, 0.03862205882528251),\n",
       " (867, 0.03628053145089198),\n",
       " (868, 0.0387854860116686),\n",
       " (869, 0.039592123320360745),\n",
       " (870, 0.03742085572572101),\n",
       " (871, 0.035344902717974726),\n",
       " (872, 0.03764788088086238),\n",
       " (873, 0.03735239487849263),\n",
       " (874, 0.037749023374219565),\n",
       " (875, 0.0381243881313767),\n",
       " (876, 0.03577916092605095),\n",
       " (877, 0.03850901522614025),\n",
       " (878, 0.03870210690736985),\n",
       " (879, 0.04022167165480584),\n",
       " (880, 0.039648012975989036),\n",
       " (881, 0.0379368134587939),\n",
       " (882, 0.036594389203742785),\n",
       " (883, 0.038184048161642226),\n",
       " (884, 0.036722895416063606),\n",
       " (885, 0.03896813091638862),\n",
       " (886, 0.03879733125359325),\n",
       " (887, 0.03830264042998268),\n",
       " (888, 0.036429009523831074),\n",
       " (889, 0.039194861473768654),\n",
       " (890, 0.040799127094503396),\n",
       " (891, 0.03743809104434009),\n",
       " (892, 0.037072489457720854),\n",
       " (893, 0.03921173572556225),\n",
       " (894, 0.03883699303238084),\n",
       " (895, 0.03523023155637217),\n",
       " (896, 0.03965494885704282),\n",
       " (897, 0.03695062523187045),\n",
       " (898, 0.0356127806101242),\n",
       " (899, 0.0366517755480635),\n",
       " (900, 0.038167599928342955),\n",
       " (901, 0.035682633691753105),\n",
       " (902, 0.03864509196851415),\n",
       " (903, 0.0397191027653275),\n",
       " (904, 0.03508750421648895),\n",
       " (905, 0.039025959021242805),\n",
       " (906, 0.03824553173450517),\n",
       " (907, 0.04091291278719329),\n",
       " (908, 0.03850515114665543),\n",
       " (909, 0.036973643804258945),\n",
       " (910, 0.03355130319716177),\n",
       " (911, 0.04014263476625363),\n",
       " (912, 0.038493345276161814),\n",
       " (913, 0.036322457948812364),\n",
       " (914, 0.035951898638351014),\n",
       " (915, 0.032532851888123104),\n",
       " (916, 0.033677669368250285),\n",
       " (917, 0.03663582378831717),\n",
       " (918, 0.03800655100291241),\n",
       " (919, 0.0368979958599302),\n",
       " (920, 0.038453126853616196),\n",
       " (921, 0.036620090455002176),\n",
       " (922, 0.03972095503726026),\n",
       " (923, 0.03601254178796406),\n",
       " (924, 0.03481678352137205),\n",
       " (925, 0.03796101161448877),\n",
       " (926, 0.03962130256303655),\n",
       " (927, 0.0341919583120932),\n",
       " (928, 0.03487442500917549),\n",
       " (929, 0.03963685265250111),\n",
       " (930, 0.04167619956923552),\n",
       " (931, 0.039268351997693),\n",
       " (932, 0.03587643721196285),\n",
       " (933, 0.03389600828715178),\n",
       " (934, 0.03682468309364415),\n",
       " (935, 0.03458350832559071),\n",
       " (936, 0.03628890671762203),\n",
       " (937, 0.038851659791304685),\n",
       " (938, 0.03535020237617672),\n",
       " (939, 0.03660230475883355),\n",
       " (940, 0.03634515509784984),\n",
       " (941, 0.038966374355904373),\n",
       " (942, 0.03727706503132112),\n",
       " (943, 0.038275030752763346),\n",
       " (944, 0.03889020118517907),\n",
       " (945, 0.03504456701144821),\n",
       " (946, 0.03720598434445803),\n",
       " (947, 0.03927030054067153),\n",
       " (948, 0.035087811751641336),\n",
       " (949, 0.0369674484923044),\n",
       " (950, 0.036461299630069555),\n",
       " (951, 0.03696671550419033),\n",
       " (952, 0.037007427829274464),\n",
       " (953, 0.03338740940711118),\n",
       " (954, 0.0357773152625635),\n",
       " (955, 0.03796342969195752),\n",
       " (956, 0.03475902075718802),\n",
       " (957, 0.03445676201313774),\n",
       " (958, 0.03924669266025113),\n",
       " (959, 0.03517324656328952),\n",
       " (960, 0.03983635940769911),\n",
       " (961, 0.03791201077897793),\n",
       " (962, 0.039661563652876736),\n",
       " (963, 0.040160044395696724),\n",
       " (964, 0.03576349647960068),\n",
       " (965, 0.037963337715633795),\n",
       " (966, 0.037797799568591514),\n",
       " (967, 0.038088531820073365),\n",
       " (968, 0.03593099435501386),\n",
       " (969, 0.03727624738275912),\n",
       " (970, 0.034681798519816336),\n",
       " (971, 0.03688654429849281),\n",
       " (972, 0.03862459424359462),\n",
       " (973, 0.03991377372976316),\n",
       " (974, 0.03805371409822161),\n",
       " (975, 0.03643338305032046),\n",
       " (976, 0.03913927724867734),\n",
       " (977, 0.03569234007055656),\n",
       " (978, 0.036744467829936574),\n",
       " (979, 0.03768417864271946),\n",
       " (980, 0.03905399643248274),\n",
       " (981, 0.03528983240426804),\n",
       " (982, 0.03848669815325204),\n",
       " (983, 0.03662895786547504),\n",
       " (984, 0.039463118281218366),\n",
       " (985, 0.039725451005972606),\n",
       " (986, 0.03898165893486775),\n",
       " (987, 0.03574590724044576),\n",
       " (988, 0.038253624652130724),\n",
       " (989, 0.03715006822500225),\n",
       " (990, 0.03696888857408995),\n",
       " (991, 0.03973498722493671),\n",
       " (992, 0.03730181645639994),\n",
       " (993, 0.03885733123461683),\n",
       " (994, 0.03504911105557002),\n",
       " (995, 0.03754937990638129),\n",
       " (996, 0.03538545875070633),\n",
       " (997, 0.03372394355088518),\n",
       " (998, 0.03881178439363776),\n",
       " (999, 0.03932152691141475)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_procc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 0 1]\n",
      " [4 0 0 1]\n",
      " [1 1 0 5]\n",
      " [1 0 0 4]\n",
      " [0 1 5 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0009324747956208"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reference\n",
    "print(R)\n",
    "\n",
    "mf.get_rating(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.98194498, 3.00093247, 2.3784337 , 1.01242681],\n",
       "       [3.99486291, 2.24952314, 2.32799669, 1.01166204],\n",
       "       [1.00799302, 1.00624237, 5.84265511, 4.98357297],\n",
       "       [1.01021892, 0.76830632, 4.90953484, 3.99582105],\n",
       "       [1.39315713, 1.01664805, 4.98871382, 3.99816089]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf.full_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
